{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33db005e-f026-4f71-8704-c621e544dfdb",
   "metadata": {},
   "source": [
    "# <font color='Crimson'><b>VARIABLE SELECTION</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a1dd2-74e3-40fe-adc0-47d2edd12c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import make_scorer\n",
    "#from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "from sklearn.metrics import class_likelihood_ratios\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998b0ee-27a2-4670-8d29-eb7f31dc574a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <font color='indianred'><b>FEATURE IMPORTANCE</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9f252-583c-49a5-8b60-3cb6c21fd3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import imputed and standardized training set:\n",
    "training_std = pd.read_csv('05_Training_standard.csv',sep=',',index_col=0)\n",
    "training_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5f31d-67c2-4ea7-a764-aed9f73f65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import imputed and standardized test set:\n",
    "test_std = pd.read_csv('05_Test_standard.csv',sep=',',index_col=0)\n",
    "test_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9880a4-0f72-44a0-9832-bf7ff1420c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the two sets:\n",
    "training_std = shuffle(training_std,random_state=100)\n",
    "test_std = shuffle(test_std,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36922be5-cbb1-4a57-a4de-5afc4bab2382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split features from outcome in both sets:\n",
    "X_training = training_std.iloc[:,2:training_std.shape[1]].values\n",
    "y_training = training_std.iloc[:,0].values\n",
    "y_training = np.double(y_training)\n",
    "X_test = test_std.iloc[:,2:test_std.shape[1]].values\n",
    "y_test = test_std.iloc[:,0].values\n",
    "y_test = np.double(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26889588-39c4-43cf-953f-b54f4b94dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define useful functions for classification task:\n",
    "#TSS score\n",
    "def my_tss_score(Y_training, probability_prediction,threshold):\n",
    "    \n",
    "    Y_predicted = probability_prediction > threshold\n",
    "    res = metrics_classification(Y_training > 0, Y_predicted, print_skills=False)    \n",
    "\n",
    "    return res['tss']\n",
    "\n",
    "#Calssification metrics\n",
    "def metrics_classification(y_real, y_pred, print_skills=True):\n",
    "\n",
    "    cm, far, pod, acc, hss, tss, fnfp, csi, tpr, tnr = classification_skills(y_real, y_pred)\n",
    "\n",
    "    if print_skills:\n",
    "        print ('confusion matrix')\n",
    "        print (cm)\n",
    "        print ('false alarm ratio       \\t', far)\n",
    "        print ('probability of detection\\t', pod)\n",
    "        print ('accuracy                \\t', acc)\n",
    "        print ('hss                     \\t', hss)\n",
    "        print ('tss                     \\t', tss)\n",
    "        print ('balance                 \\t', fnfp)\n",
    "        print ('csi                 \\t', csi)\n",
    "        print ('tpr                 \\t', tpr)\n",
    "        print ('tnr                 \\t', tnr)\n",
    "\n",
    "    balance_label = float(sum(y_real)) / y_real.shape[0]\n",
    "\n",
    "    return {\n",
    "        \"cm\": cm,\n",
    "        \"far\": far,\n",
    "        \"pod\": pod,\n",
    "        \"acc\": acc,\n",
    "        \"hss\": hss,\n",
    "        \"tss\": tss,\n",
    "        \"fnfp\": fnfp,\n",
    "        \"balance label\": balance_label,\n",
    "        \"csi\": csi,\n",
    "        \"tpr\": tpr,\n",
    "        \"tnr\": tnr}\n",
    "\n",
    "def classification_skills(y_real, y_pred):\n",
    "\n",
    "    cm = confusion_matrix(y_real, y_pred)\n",
    "\n",
    "    if cm.shape[0] == 1 and sum(y_real) == 0:\n",
    "        a = 0.\n",
    "        d = float(cm[0, 0])\n",
    "        b = 0.\n",
    "        c = 0.\n",
    "    elif cm.shape[0] == 1 and sum(y_real) == y_real.shape[0]:\n",
    "        a = float(cm[0, 0])\n",
    "        d = 0.\n",
    "        b = 0.\n",
    "        c = 0.\n",
    "    elif cm.shape[0] == 2:\n",
    "        a = float(cm[1, 1])\n",
    "        d = float(cm[0, 0])\n",
    "        b = float(cm[0, 1])\n",
    "        c = float(cm[1, 0])\n",
    "    TP = a\n",
    "    TN = d\n",
    "    FP = b\n",
    "    FN = c\n",
    "\n",
    "    if (TP + FP + FN + TN) == 0.:\n",
    "        if (TP + TN) == 0.:\n",
    "            acc = 0.  # float('NaN')\n",
    "        else:\n",
    "            acc = -100  # float('Inf')\n",
    "    else:\n",
    "        acc = (TP + TN) / (TP + FP + FN + TN)\n",
    "        \n",
    "\n",
    "    if TP + FN == 0.:\n",
    "        if TP == 0.:\n",
    "            tss_aux1 = 0.  # float('NaN')\n",
    "        else:\n",
    "            tss_aux1 = -100  # float('Inf')\n",
    "    else:\n",
    "        tss_aux1 = (TP / (TP + FN))\n",
    "\n",
    "    if (FP + TN) == 0.:\n",
    "        if FP == 0.:\n",
    "            tss_aux2 = 0.  # float('NaN')\n",
    "        else:\n",
    "            tss_aux2 = -100  # float('Inf')\n",
    "    else:\n",
    "        tss_aux2 = (FP / (FP + TN))\n",
    "\n",
    "    tss = tss_aux1 - tss_aux2\n",
    "\n",
    "    if ((TP + FN) * (FN + TN) + (TP + FP) * (FP + TN)) == 0.:\n",
    "        if (TP * TN - FN * FP) == 0:\n",
    "            hss = 0.  # float('NaN')\n",
    "        else:\n",
    "            hss = -100  # float('Inf')\n",
    "    else:\n",
    "        hss = 2 * (TP * TN - FN * FP) / ((TP + FN) *\n",
    "                                         (FN + TN) + (TP + FP) * (FP + TN))\n",
    "\n",
    "    if FP == 0.:\n",
    "        if FN == 0.:\n",
    "            fnfp = 0.  # float('NaN')\n",
    "        else:\n",
    "            fnfp = -100  # float('Inf')\n",
    "    else:\n",
    "        fnfp = FN / FP\n",
    "\n",
    "    if (TP + FN) == 0.:\n",
    "        if TP == 0.:\n",
    "            pod = 0  # float('NaN')\n",
    "        else:\n",
    "            pod = -100  # float('Inf')\n",
    "    else:\n",
    "        pod = TP / (TP + FN)\n",
    "\n",
    "\n",
    "    if (TP + FP) == 0.:\n",
    "        if FP == 0.:\n",
    "            far = 0.  # float('NaN')\n",
    "        else:\n",
    "            far = -100  # float('Inf')\n",
    "    else:\n",
    "        far = FP / (TP + FP)\n",
    "\n",
    "    #acc = (a + d) / (a + b + c + d)\n",
    "    tpr = tss_aux1  # a / (a + b)\n",
    "    tnr = 1-tss_aux2  # d / (d + c)\n",
    "    #wtpr = a / (a + b) * (a + c) / (a + b + c + d) + d / (c + d) * (b + d) / (a + b + c + d)\n",
    "    #pacc = a / (a + c)\n",
    "    #nacc = d / (b + d)\n",
    "    #wacc = a / (a + c) * (a + c) / (a + b + c + d) + d / (b + d) * (b + d) / (a + b + c + d)\n",
    "\n",
    "    # if the cm has a row or a column equal to 0, we have bad tss\n",
    "    if TP+FN == 0 or TN+FP == 0 or TP+FP == 0 or TN+FN == 0:\n",
    "        tss = 0\n",
    "    if TP+FP+FN==0:\n",
    "        csi = 0\n",
    "    else:\n",
    "        csi = TP/(TP+FP+FN)\n",
    "\n",
    "    return cm.tolist(), far, pod, acc, hss, tss, fnfp, csi, tpr, tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8d6d3-6a22-4139-a5eb-6564e198c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define useful fuctions for cross-validation:\n",
    "def build_grid(p):\n",
    "    #Parameters initialization:\n",
    "    model_name = None if p['model_name'] == None else p['model_name']\n",
    "    GRID = None \n",
    "\n",
    "    if model_name == \"RF\":\n",
    "        GRID = {'n_estimators': p['n_estimators'],\n",
    "                'max_features': p['max_features'],\n",
    "                'max_depth': p['max_depth'],\n",
    "                'criterion': p['criterion']\n",
    "               }   \n",
    "\n",
    "    #other models can be added here\n",
    "\n",
    "    return GRID\n",
    "\n",
    "\n",
    "def initiate_p(p):\n",
    "    #Ranges for hyperparameter search:\n",
    "    if p['model_name'] == 'RF':\n",
    "        p['estimator'] = RandomForestClassifier(random_state=100)\n",
    "        p['n_estimators'] = [100,200,300,400,500]\n",
    "        p['max_features'] = [None, 'sqrt', 'log2'] \n",
    "        p['max_depth'] = [4,5,6,7,8]\n",
    "        p['criterion'] = ['gini','entropy']\n",
    "\n",
    "    #other models can be added here\n",
    "\n",
    "    #All:\n",
    "    p['cv'] = 10\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def GridSearch_(X,y,p):\n",
    "    #Parameters initialization:\n",
    "    estimator = None if p['estimator'] == None else p['estimator']\n",
    "    cv = 10 if p['cv'] == None else p['cv']\n",
    "    grid = None if p['grid'] == None else p['grid']\n",
    "    tau = 0.5 if p['threshold'] == None else p['threshold']\n",
    "    \n",
    "    if(estimator == None or grid == None):\n",
    "        return None\n",
    "\n",
    "    #1st step: select the best hyperparameters\n",
    "    CV = GridSearchCV(estimator     = estimator,\n",
    "                        param_grid  = grid,\n",
    "                        scoring     =  make_scorer(my_tss_score,threshold=tau,needs_proba=True), #TSS score\n",
    "                        #refit       = 'roc_auc',\n",
    "                        cv          = cv,\n",
    "                        verbose     = 0,\n",
    "                        n_jobs      = 20,\n",
    "                        return_train_score=True)\n",
    "    CV_H = CV.fit(X,y)\n",
    "    \n",
    "    return CV_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d8778-f779-47b9-bd1a-ded30dbe53ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the function that returns the ranking of features, from the most important to the less one:\n",
    "\n",
    "#Default scoring function:\n",
    "scoring = make_scorer(my_tss_score,threshold=0.10,needs_proba=True)\n",
    "\n",
    "#Feature ranking:\n",
    "def feature_importance(model,X,y,df,n_repeats=30, random_state=100, scoring=scoring):\n",
    "    r = permutation_importance(model, X, y, n_repeats=30, random_state=100, scoring=scoring)\n",
    "    feature_rel = []\n",
    "    mean_value_features=[]\n",
    "    std_value_features=[]\n",
    "    for i in r.importances_mean.argsort()[::-1]:\n",
    "        if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "            print(f\"{df.columns[i]:<8}\"\n",
    "               f\"{r.importances_mean[i]:.3f}\"\n",
    "               f\" +/- {r.importances_std[i]:.3f}\")\n",
    "            feature_rel.append(df.columns[i])\n",
    "            mean_value_features.append(r.importances_mean[i])\n",
    "            std_value_features.append(r.importances_std[i])\n",
    "    return feature_rel,mean_value_features,std_value_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3840692a-988e-4460-b5e7-e31803250f03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Compute feature importance for each of the 3*10 folds trained before:\n",
    "#On validation set:\n",
    "dataset = 'validation' \n",
    "#Number of shuffles:\n",
    "R = 3\n",
    "#Number of folds:\n",
    "K = 10\n",
    "\n",
    "#Thresholds evaluated:\n",
    "thresholds = np.linspace(0.0,0.5,11)\n",
    "#Classifier trained:\n",
    "classifiers = ['RF'] #LogisticRegL1 #LogisticRegL2\n",
    "\n",
    "#Shuffle data in order to test stability of the model:\n",
    "#In order to guarantee reproducibility of results, the random states are always the numbers 0,1,2.\n",
    "for r in range(0,R):\n",
    "    X,y = shuffle(X_training,y_training,random_state = r) \n",
    "    for k in range(0,K):\n",
    "        for cl in classifiers:\n",
    "            p = {}\n",
    "            p['model_name'] = cl\n",
    "            p = initiate_p(p)  \n",
    "            #Read from file:\n",
    "            #insert your path here:\n",
    "            nome_file = \"<insert your path>/R\"+ str(r) + \"_K\" + str(k) + '_' + p['model_name'] + '.npy' #\"K-Folds/R\"\n",
    "            metrics = np.load(nome_file, allow_pickle = True).item()\n",
    "            \n",
    "            index = ''\n",
    "            if dataset == 'validation':\n",
    "                index = 'validation_index'\n",
    "            elif dataset == 'train':\n",
    "                index = 'train_index'\n",
    "                \n",
    "            index = metrics[index]\n",
    "            X_kfold = X[index]\n",
    "            y_kfold = y[index]\n",
    "            \n",
    "            best_thresholds = {}\n",
    "            best_threshold = -1\n",
    "            best_hyperparameters = {}\n",
    "            \n",
    "            metrics_cl = metrics[cl]\n",
    "            \n",
    "            for th in thresholds:\n",
    "                performance = metrics_cl[th]\n",
    "                \n",
    "                if dataset == 'validation':\n",
    "                    sensitivity = performance['recall_ts']\n",
    "                    specificity = performance['specificity_ts']\n",
    "                    tss = performance['tss_ts']\n",
    "                elif dataset == 'train':\n",
    "                    sensitivity = performance['recall_tr']\n",
    "                    specificity = performance['specificity_tr']\n",
    "                    tss = performance['tss_tr']\n",
    "                \n",
    "                #Relationship of interest:\n",
    "                if sensitivity >= specificity:\n",
    "                    if round(specificity,2) >= 0.55:\n",
    "                        if round(sensitivity,2) >= 0.65: \n",
    "                            best_thresholds[th] = tss\n",
    "                \n",
    "            if len(best_thresholds) == 0:\n",
    "                print('No threshold satisfies the relationship!')\n",
    "            elif len(best_thresholds) == 1:\n",
    "                for key, value in best_thresholds.items():\n",
    "                    best_threshold = key\n",
    "            else:\n",
    "                best_threshold = max(best_thresholds, key=best_thresholds.get)\n",
    "\n",
    "            if best_threshold == -1:\n",
    "                fi = {}\n",
    "                fi['Features_names'] = []\n",
    "                fi['Mean_values'] = []\n",
    "                fi['Std_values'] = []\n",
    "                #nome_file = \"<insert your path>/R\"+ str(r) + \"_K\" + str(k) + '_' + p['model_name'] + '_FeaturesImportance_'+dataset + '.npy' \n",
    "                #np.save(nome_file, fi) \n",
    "                continue\n",
    "                \n",
    "            print(\"Best threshold:\",best_threshold)\n",
    "            \n",
    "            performance = metrics_cl[best_threshold]\n",
    "            \n",
    "            best_hyperparameters = performance['best_hyper']\n",
    "            \n",
    "            print(\"Best set of hyperparameters:\",best_hyperparameters)\n",
    "\n",
    "            for key, value in best_hyperparameters.items():\n",
    "                p[key] = value\n",
    "\n",
    "            scoring = make_scorer(my_tss_score,threshold=best_threshold,needs_proba=True)\n",
    "\n",
    "            if p['model_name'] == 'LogisticRegL1':\n",
    "                fit = LogisticRegressionCV(cv = 1,random_state = 100,penalty='l1',solver='liblinear',scoring=scoring).fit(X_kfold, y_kfold) \n",
    "\n",
    "            elif p['model_name'] == 'LogisticRegL2':\n",
    "                fit = LogisticRegressionCV(cv = 1,random_state = 100,penalty='l2',scoring=scoring).fit(X_kfold, y_kfold) \n",
    "\n",
    "            elif p['model_name'] == 'RF':\n",
    "                max_depth = p['max_depth']\n",
    "                n_estimators = p['n_estimators']\n",
    "                criterion = p['criterion']\n",
    "                max_features = p['max_features']\n",
    "                fit = RandomForestClassifier(max_depth=max_depth,n_estimators=n_estimators,criterion=criterion,\n",
    "                                             max_features=max_features,random_state=100).fit(X_kfold, y_kfold) \n",
    "                \n",
    "            #other models can be added here\n",
    "                \n",
    "                \n",
    "            feature_rel,mean_value,std_value = feature_importance(fit,X_kfold,y_kfold,\n",
    "                                                                  train_std.iloc[:,2:training_std.shape[1]],\n",
    "                                                                  n_repeats=30, random_state=100, \n",
    "                                                                  scoring=scoring)  \n",
    "            \n",
    "            fi = {}\n",
    "            fi['Features_names'] = feature_rel\n",
    "            fi['Mean_values'] = mean_value\n",
    "            fi['Std_values'] = std_value\n",
    "            #df_fi = pd.DataFrame(fi)\n",
    "            #Save on file:\n",
    "            #nome_file = \"<insert your path>/R\"+ str(r) + \"_K\" + str(k) + '_' + p['model_name'] + '_FeaturesImportance_'+dataset + '.npy'\n",
    "            #np.save(nome_file, fi)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f76d0-7ce6-47d6-b8dd-7eb358510f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate all the ranks and the best position reached by each feature:\n",
    "all_features = ['SESSO','30gg','ETA','BASOFILI','EOSINOFILI','LINFOCITI','MONOCITI','NEUTROFILI','EMATOCRITO','EMOGLOBINA',\n",
    "                'GLOBULI_B','GLOBULI_R','PIASTRINE','APTT','INR','TEMPO_PROTROMB','ACIDO_URICO','ALP','ALT','AST',\n",
    "                'BILIRUBINA_TOT','CREATININA','GGT','LAD','UREA','GLUCOSIO','ALBUMINA','PCR','PROTEINE_TOT']\n",
    "\n",
    "#Best ranks:\n",
    "best_rank = {} #Key:Feature, Value: Best rank\n",
    "#All ranks:\n",
    "all_ranks = {} #Key: Feature, Value:Vocabulary -> 1:n, 2:n, ...\n",
    "ranks = {}\n",
    "for i in range(1,len(all_features)+1):\n",
    "    ranks[i] = 0\n",
    "\n",
    "#Initialization of the two dictionaries' keys:\n",
    "for f in all_features:\n",
    "    best_rank[f] = 100\n",
    "    \n",
    "    all_ranks[f] = ranks\n",
    "\n",
    "#Update dictionaries:\n",
    "for r in range(0,R):\n",
    "    for k in range(0,K):\n",
    "        for cl in classifiers:\n",
    "            model_name = cl   \n",
    "            #Read from file:\n",
    "            #insert your path here:\n",
    "            nome_file = \"<insert your path>/R\"+ str(r) + \"_K\" + str(k) + '_' + p['model_name'] + '_FeaturesImportance_'+dataset + '.npy' \n",
    "            fi = np.load(nome_file, allow_pickle = True).item()\n",
    "            Features_names = fi['Features_names']\n",
    "            \n",
    "            rank = 1\n",
    "            for name in Features_names:\n",
    "                if best_rank[name] > rank:\n",
    "                    best_rank[name] = rank\n",
    "                                \n",
    "                if rank <= len(all_features):\n",
    "                    ranks = all_ranks[name].copy()\n",
    "                    all_ranks[name] = ranks\n",
    "                    ranks[rank] += 1\n",
    "                    \n",
    "                rank += 1\n",
    "                \n",
    "print(best_rank)\n",
    "print(all_ranks)\n",
    "#Save on file:\n",
    "#nome_file = \"<insert your path>/best_rank.npy\"\n",
    "#np.save(nome_file, best_rank)   \n",
    "#nome_file = \"<insert your path>/all_ranks.npy\"\n",
    "#np.save(nome_file, all_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb46602-9ec7-4fa7-9652-82192e0a4207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your path in the following 2 lines:\n",
    "#best_rank = np.load('<insert your path>/best_rank.npy', allow_pickle = True).item()\n",
    "all_ranks = np.load('<insert your path>/all_ranks.npy', allow_pickle = True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc978d-634e-4b6b-9996-a92494675f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change column names for Figure S3:\n",
    "all_features = ['SESSO','30gg','ETA','BASOFILI','EOSINOFILI','LINFOCITI','MONOCITI','NEUTROFILI','EMATOCRITO','EMOGLOBINA',\n",
    "                'GLOBULI_B','GLOBULI_R','PIASTRINE','APTT','INR','TEMPO_PROTROMB','ACIDO_URICO','ALP','ALT','AST',\n",
    "                'BILIRUBINA_TOT','CREATININA','GGT','LAD','UREA','GLUCOSIO','ALBUMINA','PCR','PROTEINE_TOT']\n",
    "new_keys = [\"Sex\",\"Candida colonization\",\"Age\",\"Basophil cells count\",\"Eosinophil cells count\",\n",
    "            \"Lymphocyte cells count\",\"Monocyte cells count\",\"Neutrophil cells count\",\"Hematocrit\",\"Hemoglobin\",\n",
    "            \"White cells count\",\"Red cells count\", \"Platelets count\",\"aPTT\",\"INR\",\"Prothrombin time\",\"Uric acid\",\n",
    "            \"ALP\",\"ALT\",\"AST\",\"Total bilirubin\",\"Creatinine\",\"GGT\",\"LDH\",\"Urea\",\"Glucose\",\"Albumin\",\"CRP\",\"Total proteins\"]\n",
    "    \n",
    "def change_dict_key(d, old_key, new_key, default_value=None):\n",
    "    d[new_key] = d.pop(old_key, default_value)\n",
    "    \n",
    "for i in range(0,29):\n",
    "    change_dict_key(all_ranks, all_features[i], new_keys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e4c7c-2ada-4bba-bbba-6267efb69554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Visulize histograms for all ranks (Figure S3):\n",
    "#Add to the dictionary 'all_ranks' the number of times each feature is not important \n",
    "#(therefore is not displaied by feature importance). This number has key 100\n",
    "#all_ranks_nan = all_ranks.copy() \n",
    "#for key,val in all_ranks_nan.items():\n",
    "    #val.update({100: 23 - sum(list(val.values()))}) # 23 is the number of non-empty files\n",
    "    \n",
    "#Histograms (Figure S3):\n",
    "for key,value in all_ranks.items():\n",
    "    pos = np.arange(len(value.keys()))\n",
    "    width = 0.5     \n",
    "\n",
    "    ax = plt.axes()\n",
    "    ax.set_box_aspect(0.5)\n",
    "    ax.set_ylim(0.0,27.0)\n",
    "    yticks = [0,5,10,15,20,23]\n",
    "    ax.set_yticklabels(yticks,fontdict={'fontsize' : 6.5})\n",
    "    ax.set_xlim(0.0,30)\n",
    "    ax.set_xticks(pos + width*2)\n",
    "    ax.set_xticklabels(list(value.keys()),fontdict={'fontsize' : 6.5})\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Frequency')    \n",
    "    ax.set_title(key)\n",
    "\n",
    "    plt.bar(value.keys(),value.values(), width, color='#80b1d3')\n",
    "    \n",
    "    #nome_figura = '<insert your path>/histogram_rank_' + key + '.pdf'\n",
    "    #plt.savefig(nome_figura,dpi = 300,bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c189e6-c33d-49ee-9d60-92f402522b2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <font color='indianred'><b>SUBSET WITH BDG AND PCT NON-MISSING</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e250b096-56ec-4e52-91e0-5223b74879b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset:\n",
    "df_complete1 = pd.read_csv('02_1_Dataset.csv', sep=',',index_col=0)\n",
    "df_complete1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11dfa8-12c4-4822-95d0-056469cc22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select BDG and PCT columns:\n",
    "df_complete1_BDG_PCT = df_complete1.iloc[:,[40,42]]\n",
    "print('The number of episodes with BDG and PCT non-missing is:', \n",
    "      df_complete1_BDG_PCT[~(df_complete1_BDG_PCT['PROCALCITONINA'].isna() | df_complete1_BDG_PCT['B_D_GLUCANO'].isna())].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c48a21-dbbc-4d8d-ad58-800876c58bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import training features:\n",
    "X_training = pd.read_csv('03_X_training.csv',sep=',',index_col=0)\n",
    "X_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3868561-c58e-4a18-aff9-8c279c3916a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import training outcome:\n",
    "y_training = pd.read_csv('03_y_training.csv',sep=',',index_col=0)\n",
    "y_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d215c3-9dee-4078-90c5-27bd79d89163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import test features:\n",
    "X_test = pd.read_csv('03_X_test.csv',sep=',',index_col=0)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f174f84-9bb9-4332-bc8e-e28cf580f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import test outcome:\n",
    "y_test = pd.read_csv('03_y_test.csv',sep=',',index_col=0)\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9072b4b1-26ab-4ea3-a3d3-8da4c2483229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge training features and outcome in training set:\n",
    "training = X_training.drop(X_training.columns[[0,3,4,5,8]], axis=1)\n",
    "training['CANDIDEMIA'] = y_training\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc0ff7-26d3-4595-9c6d-133426e7bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the training set and BDG and PCT columns:\n",
    "df_complete1_training = df_complete1_BDG_PCT.merge(training, how='right',right_index=True,left_index=True)\n",
    "df_complete1_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f91a6-4b9b-40f2-a0e2-f347ba0af1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete episodes with missing BDG or PCT:\n",
    "training_BDG_PCT = df_complete1_training[~(df_complete1_training['PROCALCITONINA'].isna() | df_complete1_training['B_D_GLUCANO'].isna())]\n",
    "print('Number of samples in the new training set:',training_BDG_PCT.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11a73b-5515-46fb-b749-6b7ba7e5ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Are missing values left in the training set?',training_BDG_PCT.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050442d-d8ec-488b-8af7-7ca3e09147cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation\n",
    "#Definition of the imputer:\n",
    "imputer = IterativeImputer(KNeighborsRegressor(n_neighbors=5),\n",
    "                           sample_posterior=False, \n",
    "                           max_iter=100,\n",
    "                           tol=0.05,\n",
    "                           n_nearest_features=None,\n",
    "                           initial_strategy='most_frequent',\n",
    "                           imputation_order='random',\n",
    "                           random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4433e6c-fe45-49b2-abe9-be6bd20da3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit of the imputer on training set and imputation:\n",
    "training_BDG_PCT_imputed = imputer.fit_transform(training_BDG_PCT)\n",
    "training_BDG_PCT_imputed = pd.DataFrame(training_BDG_PCT_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2df81a-519a-4008-b1aa-fe9adfdf16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Are missing values left in the training set?',training_BDG_PCT_imputed.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ac8ac-0b4f-4c58-80e5-17d4d0d5baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Work on imputed training set:\n",
    "training_BDG_PCT_imputed.columns = training_BDG_PCT.columns\n",
    "training_BDG_PCT_imputed['30gg'] = training_BDG_PCT_imputed['30gg'] > 0.5\n",
    "training_BDG_PCT_imputed['30gg'] = training_BDG_PCT_imputed['30gg']*1.\n",
    "training_BDG_PCT_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0dfd5-ed35-4a1c-93b1-fc1406eab4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization\n",
    "#Change the order of the columns of the imputed training set:\n",
    "training_BDG_PCT_imputed = training_BDG_PCT_imputed.reindex(columns=['CANDIDEMIA','MISTA','SESSO','30gg','B_D_GLUCANO','PROCALCITONINA','ETA','BASOFILI',\n",
    "                                                                     'EOSINOFILI','LINFOCITI','MONOCITI','NEUTROFILI','EMATOCRITO','EMOGLOBINA','GLOBULI_B',\n",
    "                                                                     'GLOBULI_R','PIASTRINE','APTT','INR','TEMPO_PROTROMB','ACIDO_URICO','ALP','ALT','AST',\n",
    "                                                                     'BILIRUBINA_TOT','CREATININA','GGT','LAD','UREA','GLUCOSIO','ALBUMINA','PCR','PROTEINE_TOT'])\n",
    "training_BDG_PCT_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f433d245-3b7f-464e-b0bc-89c2bd787235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the scaler:\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432ae41-7daa-4636-ae9a-84bc3b6f4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit of the scaler on the continuous features of the training set and scale:\n",
    "X_training_BDG_PCT_std = pd.DataFrame(scaler.fit_transform(training_BDG_PCT_imputed.iloc[:,4:training_BDG_PCT_imputed.shape[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19425f-19f8-47e8-9adf-8389702e3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add categorical features to the one just scaled:\n",
    "training_BDG_PCT_std = pd.DataFrame(np.concatenate((training_BDG_PCT_imputed.iloc[:,0:4], X_training_BDG_PCT_std), axis=1))\n",
    "#Change column names:\n",
    "training_BDG_PCT_std.columns = training_BDG_PCT_imputed.columns\n",
    "training_BDG_PCT_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbac480-d144-4521-bd0d-c2895007b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge test features and outcome in training set:\n",
    "test = X_test.drop(X_test.columns[[0,3,4,5,8]], axis=1)\n",
    "test['CANDIDEMIA'] = y_test\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4cb72c-12c7-4313-9d88-956ad0fccea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the test set and BDG and PCT columns:\n",
    "df_complete1_test = df_complete1_BDG_PCT.merge(test, how='right',right_index=True,left_index=True)\n",
    "df_complete1_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb65749-b4df-474c-a076-6ea54f845cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete episodes with missing BDG or PCT:\n",
    "test_BDG_PCT = df_complete1_test[~(df_complete1_test['PROCALCITONINA'].isna() | df_complete1_test['B_D_GLUCANO'].isna())]\n",
    "print('Number of samples in the new test set:',test_BDG_PCT.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28667a-3aed-422c-b3ef-577ebd34d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Are missing values left in the test set?',test_BDG_PCT.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843ba65-f5b2-44a5-8b65-3709b07ee58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation\n",
    "#Imputation of the test set:\n",
    "test_BDG_PCT_imputed = imputer.transform(test_BDG_PCT)\n",
    "test_BDG_PCT_imputed = pd.DataFrame(test_BDG_PCT_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833eda78-4859-47ef-ba6c-b156d7eb8a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Are missing values left in the test set?',test_BDG_PCT_imputed.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3c7bb-8360-40ae-aa54-563a35a04eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Work on imputed test set:\n",
    "test_BDG_PCT_imputed.columns = test_BDG_PCT.columns\n",
    "test_BDG_PCT_imputed['30gg'] = test_BDG_PCT_imputed['30gg'] > 0.5\n",
    "test_BDG_PCT_imputed['30gg'] = test_BDG_PCT_imputed['30gg']*1.\n",
    "test_BDG_PCT_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a4061-eac6-49fb-b0a8-fb1a5f941d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization\n",
    "#Change the order of the columns of the imputed test set:\n",
    "test_BDG_PCT_imputed = test_BDG_PCT_imputed.reindex(columns=['CANDIDEMIA','MISTA','SESSO','30gg','B_D_GLUCANO','PROCALCITONINA','ETA','BASOFILI','EOSINOFILI',\n",
    "                                                             'LINFOCITI','MONOCITI','NEUTROFILI','EMATOCRITO','EMOGLOBINA','GLOBULI_B','GLOBULI_R','PIASTRINE',\n",
    "                                                             'APTT','INR','TEMPO_PROTROMB','ACIDO_URICO','ALP','ALT','AST','BILIRUBINA_TOT','CREATININA','GGT',\n",
    "                                                             'LAD','UREA','GLUCOSIO','ALBUMINA','PCR','PROTEINE_TOT'])\n",
    "test_BDG_PCT_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c4e567-8e24-4757-b289-ef926ec67419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the continuous features of the test set:\n",
    "X_test_BDG_PCT_std = pd.DataFrame(scaler.transform(test_BDG_PCT_imputed.iloc[:,4:test_BDG_PCT_imputed.shape[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbee272-6492-4c92-81f7-7939d0093d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add categorical features to the one just scaled:\n",
    "test_BDG_PCT_std = pd.DataFrame(np.concatenate((test_BDG_PCT_imputed.iloc[:,0:4], X_test_BDG_PCT_std), axis=1))\n",
    "#Change column names:\n",
    "test_BDG_PCT_std.columns = test_BDG_PCT_imputed.columns\n",
    "test_BDG_PCT_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2e8c8-7d8d-4fbe-a2f6-18907503ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New percentage of candidemias:\n",
    "print('Number of candidemias in the new training set:', pd.value_counts(training_BDG_PCT_std['CANDIDEMIA'])[1], \n",
    "      '(',round(pd.value_counts(training_BDG_PCT_std['CANDIDEMIA'], normalize=True)[1]*100,2),'%) of',\n",
    "      training_BDG_PCT_std['CANDIDEMIA'].shape[0], 'samples')\n",
    "print('--------------------------------------------------------------')\n",
    "print('Number of candidemias in the new test set:', pd.value_counts(test_BDG_PCT_std['CANDIDEMIA'])[1], \n",
    "      '(',round(pd.value_counts(test_BDG_PCT_std['CANDIDEMIA'], normalize=True)[1]*100,2),'%) of',\n",
    "      test_BDG_PCT_std['CANDIDEMIA'].shape[0], 'samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d305bd-5972-47ae-b0ed-dd126fe5f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the two sets:\n",
    "training_BDG_PCT_std = shuffle(training_BDG_PCT_std,random_state=100)\n",
    "test_BDG_PCT_std = shuffle(test_BDG_PCT_std,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292266f-876a-4529-b56c-00f4d4e1d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save on csv file:\n",
    "training_BDG_PCT_std.to_csv('07_Training_BDG_PCT_standard.csv',sep=',') \n",
    "test_BDG_PCT_std.to_csv('07_Test_BDG_PCT_standard.csv',sep=',') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c743241-51c6-4b2e-9342-7446b0c27a73",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <font color='indianred'><b>WITH FEATURES SELECTED BASED ON ALL THEIR RANKS</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad0eff-9910-4f85-933a-f591cc19b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import imputed and standardized training set:\n",
    "training_BDG_PCT_std = pd.read_csv('07_Training_BDG_PCT_standard.csv',sep=',',index_col=0) \n",
    "training_BDG_PCT_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fdcbbe-6169-43e3-8f69-3a9524f92a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import imputed and standardized test set:\n",
    "test_BDG_PCT_std= pd.read_csv('07_Test_BDG_PCT_standard.csv',sep=',',index_col=0) \n",
    "test_BDG_PCT_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a759c3-2581-417c-8f60-0ea8cfc3327c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#insert your path here:\n",
    "all_ranks = np.load('<insert your path>/all_ranks.npy', allow_pickle = True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea110374-021e-46b5-92fc-70124f71bc48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Count the number of times that each feature is in one of the first N positions\n",
    "#Choice for N: \n",
    "N = 18 #chosen looking at the previous histograms\n",
    "\n",
    "#Delete from the dictionary all_ranks values with key > N:\n",
    "all_ranks_N = all_ranks.copy()\n",
    "for key,val in all_ranks_N.items():\n",
    "    for i in range(N+1,30):\n",
    "        del all_ranks_N[key][i] \n",
    "\n",
    "n_importance_N = {} #Key:feature, Value:n\n",
    "for key,val in all_ranks_N.items():\n",
    "    n_importance_N[key] = sum(list(val.values()))\n",
    "\n",
    "subsets = {} # Key:i, Value:[features]\n",
    "for i in range(30,-1,-1):\n",
    "    subset = {}\n",
    "    for feature,n in n_importance_N.items():\n",
    "        if n >= i:\n",
    "            subset[feature] = n\n",
    "    features = list(subset.keys())\n",
    "    subsets[i] = features   \n",
    "\n",
    "#Delete duplicates from the previous dictionary:\n",
    "temp = []\n",
    "subsets_unique = dict()\n",
    "\n",
    "for key, val in subsets.items():\n",
    "    if val not in temp:\n",
    "        temp.append(val)\n",
    "        subsets_unique[key] = val\n",
    "        \n",
    "#print(subsets_unique)\n",
    "\n",
    "for key, val in subsets_unique.items():        \n",
    "    #Add the feature subset to Candidemia, BDG and PCT in both sets:\n",
    "    sub2_BDG_PCT_y_training = training_BDG_PCT_std[['CANDIDEMIA','B_D_GLUCANO','PROCALCITONINA']+val]\n",
    "    sub2_BDG_PCT_y_test = test_BDG_PCT_std[['CANDIDEMIA','B_D_GLUCANO','PROCALCITONINA']+val]\n",
    "\n",
    "    #Splitting features and outcome:\n",
    "    X_training = sub2_BDG_PCT_y_training.iloc[:,1:sub2_BDG_PCT_y_training.shape[1]].values\n",
    "    y_training = sub2_BDG_PCT_y_training.iloc[:,0].values\n",
    "    y_training = np.double(y_training)\n",
    "    X_test = sub2_BDG_PCT_y_test.iloc[:,1:sub2_BDG_PCT_y_test.shape[1]].values\n",
    "    y_test = sub2_BDG_PCT_y_test.iloc[:,0].values\n",
    "    y_test = np.double(y_test)\n",
    "\n",
    "    #Range of threshold to evaluate:\n",
    "    thresholds = np.linspace(0.0,0.5,11) \n",
    "\n",
    "    #Classifiers to train:\n",
    "    classifiers = ['RF'] #LogisticRegL1 #LogisticRegL2\n",
    "    metrics_cl = {}\n",
    "\n",
    "    for cl in classifiers:\n",
    "        p = {}\n",
    "        p['model_name'] = cl\n",
    "        p = initiate_p(p)\n",
    "        print('Model name:', p['model_name'])\n",
    "\n",
    "        metrics_th = {}\n",
    "\n",
    "        for th in tqdm(thresholds):\n",
    "            p['threshold'] = th\n",
    "            print('Threshold:', p['threshold'])\n",
    "\n",
    "            performance = {}\n",
    "            performance['features'] = sub2_BDG_PCT_y_training.columns\n",
    "            #---------------------------------------------\n",
    "            #Hyperparameters search and model fit on training set:    \n",
    "            if p['model_name'] == 'LogisticRegL1':\n",
    "                score = make_scorer(my_tss_score,threshold=th,needs_proba=True)\n",
    "                fit = LogisticRegressionCV(cv = 10,random_state = 100,penalty='l1',solver='liblinear',scoring=score).fit(X_training, y_training) \n",
    "\n",
    "            elif p['model_name'] == 'LogisticRegL2':\n",
    "                score = make_scorer(my_tss_score,threshold=th,needs_proba=True)\n",
    "                fit = LogisticRegressionCV(cv = 10,random_state = 100,penalty='l2',scoring=score).fit(X_training, y_training) \n",
    "\n",
    "            elif p['model_name'] == 'RF':\n",
    "                p['grid'] = build_grid(p)\n",
    "                p['GS_RF'] = GridSearch_(X_training, y_training, p)\n",
    "                max_depth = p['GS_RF'].best_params_['max_depth']\n",
    "                n_estimators = p['GS_RF'].best_params_['n_estimators']\n",
    "                criterion = p['GS_RF'].best_params_['criterion']\n",
    "                max_features = p['GS_RF'].best_params_['max_features']\n",
    "                print(p['GS_RF'].best_params_)\n",
    "                performance['best_hyper'] = p['GS_RF'].best_params_\n",
    "                fit = RandomForestClassifier(max_depth=max_depth,n_estimators=n_estimators,\n",
    "                                             criterion=criterion,max_features=max_features,random_state=100).fit(X_training, y_training) \n",
    "\n",
    "            #other models can be added here\n",
    "            \n",
    "            #---------------------------------------------\n",
    "            #Predict on training set:\n",
    "            PRED_prob_tr = fit.predict_proba(X_training)\n",
    "            PRED_tr_yes = PRED_prob_tr[:,1] \n",
    "\n",
    "            PRED_tr_bin = PRED_tr_yes > th\n",
    "            PRED_tr_bin = PRED_tr_bin*1.\n",
    "\n",
    "            performance['ytr'] = y_training\n",
    "            performance['PRED_prob_tr'] = PRED_prob_tr\n",
    "\n",
    "            #Compute evaluation metrics on training set:\n",
    "            print(confusion_matrix(y_training,PRED_tr_bin))\n",
    "            tn_tr, fp_tr, fn_tr, tp_tr = confusion_matrix(y_training, PRED_tr_bin).ravel()\n",
    "            spec_tr = tn_tr / (tn_tr+fp_tr)\n",
    "            f1_tr = f1_score(y_training, PRED_tr_bin,average = 'weighted')\n",
    "            acc_tr = accuracy_score(y_training, PRED_tr_bin)\n",
    "            prec_tr = precision_score(y_training, PRED_tr_bin,average = 'weighted')\n",
    "            recall_tr = recall_score(y_training, PRED_tr_bin) \n",
    "            npv_tr = tn_tr / (tn_tr+fn_tr)\n",
    "\n",
    "            performance['tss_tr'] = recall_tr+spec_tr-1\n",
    "            performance['f1score_tr'] = f1_tr\n",
    "            performance['accuracy_tr'] = acc_tr\n",
    "            performance['precision_tr'] = prec_tr\n",
    "            performance['recall_tr'] = recall_tr\n",
    "            performance['specificity_tr'] = spec_tr\n",
    "            performance['npv_tr'] = npv_tr \n",
    "\n",
    "            #---------------------------------------------\n",
    "            #Predict on test set:\n",
    "            PRED_prob_ts = fit.predict_proba(X_test)\n",
    "            PRED_ts_yes = PRED_prob_ts[:,1]\n",
    "\n",
    "            PRED_ts_bin = PRED_ts_yes>th\n",
    "            PRED_ts_bin = PRED_ts_bin*1.\n",
    "\n",
    "            performance['yts'] = y_test\n",
    "            performance['PRED_prob_ts'] = PRED_prob_ts\n",
    "\n",
    "            #Compute evaluation metrics on test set:\n",
    "            print(confusion_matrix(y_test,PRED_ts_bin))\n",
    "            tn_ts, fp_ts, fn_ts, tp_ts = confusion_matrix(y_test,PRED_ts_bin).ravel()\n",
    "            spec_ts = tn_ts / (tn_ts+fp_ts)\n",
    "            f1_ts = f1_score(y_test, PRED_ts_bin,average = 'weighted')\n",
    "            acc_ts = accuracy_score(y_test, PRED_ts_bin)\n",
    "            prec_ts = precision_score(y_test, PRED_ts_bin,average = 'weighted')\n",
    "            recall_ts = recall_score(y_test, PRED_ts_bin)  \n",
    "            npv_ts = tn_ts / (tn_ts+fn_ts)\n",
    "\n",
    "            performance['tss_ts'] = recall_ts+spec_ts-1\n",
    "            performance['f1score_ts'] = f1_ts\n",
    "            performance['accuracy_ts'] = acc_ts\n",
    "            performance['precision_ts'] = prec_ts\n",
    "            performance['recall_ts'] = recall_ts\n",
    "            performance['specificity_ts'] = spec_ts\n",
    "            performance['npv_ts'] = npv_ts \n",
    "            #---------------------------------------------\n",
    "            metrics_th[th] = performance\n",
    "        #---------------------------------------------\n",
    "        #Performances at each treshold:\n",
    "        metrics_cl[cl] = metrics_th\n",
    "        #Save on file:\n",
    "        #nome_file = '<insert your path>/Cutoff' + str(N) + '_' + cl + '_Performances_rank'+ str(key)+'.npy' \n",
    "        #np.save(nome_file, metrics_th)\n",
    "    #Save on file:\n",
    "    #nome_file = '<insert your path>/Complete_Performances_Cutoff_' + str(N)+'.npy'\n",
    "    #np.save(nome_file, metrics_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc00751e-a61a-46ee-bce1-6bc59ffb5568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#insert your path here:\n",
    "all_ranks = np.load('<insert your path>/all_ranks.npy', allow_pickle = True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99278e0-3468-4db3-acbb-a1107d076ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performances visualization (TSS, accuracy, F1-score, precision, NPV) on training set (Figure S4):\n",
    "N = [30,23,22,21,20,19,18,16,15,13,12,10,9,8,6,0] #key of subset_unique\n",
    "thresholds = np.linspace(0.0,0.5,11)\n",
    "\n",
    "for th in thresholds:\n",
    "    n = [] #number of features plus BDG and PCT\n",
    "    tss = []\n",
    "    f1 = []\n",
    "    acc = []\n",
    "    prec = []\n",
    "    se = []\n",
    "    sp = []\n",
    "    npv = []\n",
    "    #lr_p = []\n",
    "    #lr_n = []\n",
    "    \n",
    "    for i in N:\n",
    "        #insert your path here:\n",
    "        nome_file = '<insert your path>/Cutoff18_RF_Performances_rank' + str(i) +'.npy'\n",
    "        file = np.load(nome_file, allow_pickle = True).item()\n",
    "        metrics = file[th]\n",
    "        \n",
    "        n.append(len(metrics['features'])-3)\n",
    "        tss.append(metrics['tss_tr'])\n",
    "        f1.append(metrics['f1score_tr'])\n",
    "        acc.append(metrics['accuracy_tr'])\n",
    "        prec.append(metrics['precision_tr'])\n",
    "        npv.append(metrics['npv_tr'])\n",
    "        se.append(metrics['recall_tr'])\n",
    "        sp.append(metrics['specificity_tr'])\n",
    "        \n",
    "        #Likelihood Ratios:\n",
    "        #pos_LR_tr, neg_LR_tr = class_likelihood_ratios(metrics['ytr'],PRED_tr_bin)\n",
    "        #lr_p.append(pos_LR_tr)\n",
    "        #lr_n.append(neg_LR_tr)\n",
    "        \n",
    "    #performances to visualize:\n",
    "    y = np.array([tss,f1,acc,prec,npv]).transpose() #,lr_p,lr_n\n",
    "    plt.xlabel(\"N\")\n",
    "    plt.ylabel(\"Model performance\")\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    #plt.title(str(th))\n",
    "\n",
    "    legend = ['TSS', 'F1-score', 'Accuracy', 'Precision','NPV'] #, \"LR+\",\"LR-\"\n",
    "    colors = ['#fdb462','#b3de69','#bebada','#fb8072','#80b1d3'] #,'#8dd3c7','#ffffb3'\n",
    "\n",
    "    for i in range(len(y[0])):\n",
    "        plt.plot(n,[pt[i] for pt in y],label = legend[i], color = colors[i])\n",
    "    plt.legend(loc=(0.82,0.22),fontsize='x-small')\n",
    "    plt.xticks(n,size=6.5)\n",
    "    plt.yticks(size=6.5)\n",
    "    #nome_figura = '<insert your path>/Model_tss_f1_acc_prec_npv_trend_' + str(th) +'.png'\n",
    "    #plt.savefig(nome_figura,dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a88bcb-6df2-4bbf-bd5d-06fe8cc67609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performances visualization (recall, SP) on training set (Figure S4):\n",
    "N = [30,23,22,21,20,19,18,16,15,13,12,10,9,8,6,0] #key of subset_unique\n",
    "thresholds = np.linspace(0.0,0.5,11)\n",
    "\n",
    "for th in thresholds:\n",
    "    n = [] #number of features plus BDG and PCT\n",
    "    tss = []\n",
    "    f1 = []\n",
    "    acc = []\n",
    "    prec = []\n",
    "    se = []\n",
    "    sp = []\n",
    "    \n",
    "    for i in N:\n",
    "        #insert your pat here:\n",
    "        nome_file = '<insert your path>/Cutoff18_RF_Performances_rank' + str(i) +'.npy'\n",
    "        file = np.load(nome_file, allow_pickle = True).item()\n",
    "        metrics = file[th]\n",
    "        \n",
    "        n.append(len(metrics['features'])-3)\n",
    "        tss.append(metrics['tss_tr'])\n",
    "        f1.append(metrics['f1score_tr'])\n",
    "        acc.append(metrics['accuracy_tr'])\n",
    "        prec.append(metrics['precision_tr'])\n",
    "        npv.append(metrics['npv_tr'])\n",
    "        se.append(metrics['recall_tr'])\n",
    "        sp.append(metrics['specificity_tr'])\n",
    "        \n",
    "        #Likelihood Ratios:\n",
    "        #pos_LR_tr, neg_LR_tr = class_likelihood_ratios(metrics['ytr'],PRED_tr_bin)\n",
    "        #lr_p.append(pos_LR_tr)\n",
    "        #lr_n.append(neg_LR_tr)\n",
    "        \n",
    "    #performances to visualize:\n",
    "    y = np.array([se,sp]).transpose() \n",
    "    plt.xlabel(\"N\")\n",
    "    plt.ylabel(\"Model performance\")\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    #plt.title(str(th))\n",
    "\n",
    "    legend = ['Recall', 'Specificity']\n",
    "    colors = ['#8dd3c7','#fff033']\n",
    "\n",
    "    for i in range(len(y[0])):\n",
    "        plt.plot(n,[pt[i] for pt in y],label = legend[i], color = colors[i])\n",
    "    plt.legend(loc=(0.82,0.22),fontsize='x-small')\n",
    "    plt.xticks(n,size=6.5)\n",
    "    plt.yticks(size=6.5)\n",
    "    plt.axhline(y=0.6, color='black', linestyle='-.',linewidth=0.7)\n",
    "    #nome_figura = '<insert your path>/Model_se_sp_trend_' + str(th) +'.png'\n",
    "    #plt.savefig(nome_figura,dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6fdc8e-dc35-4e46-a9e3-5031a9ce7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zoom in threshold between 0.15 and 0.2:\n",
    "N = 18 #chosen looking at the previous histograms\n",
    "\n",
    "#Delete from the dictionary all_ranks values with key > N:\n",
    "all_ranks_N = all_ranks.copy()\n",
    "for key,val in all_ranks_N.items():\n",
    "    for i in range(N+1,30):\n",
    "        del all_ranks_N[key][i] \n",
    "\n",
    "n_importance_N = {} #Key:feature, Value:n\n",
    "for key,val in all_ranks_N.items():\n",
    "    n_importance_N[key] = sum(list(val.values()))\n",
    "\n",
    "subsets = {} # Key:i, Value:[features]\n",
    "for i in range(30,-1,-1):\n",
    "    subset = {}\n",
    "    for feature,n in n_importance_N.items():\n",
    "        if n >= i:\n",
    "            subset[feature] = n\n",
    "    features = list(subset.keys())\n",
    "    subsets[i] = features   \n",
    "\n",
    "#Delete duplicates from the previous dictionary:\n",
    "temp = []\n",
    "subsets_unique = dict()\n",
    "\n",
    "for key, val in subsets.items():\n",
    "    if val not in temp:\n",
    "        temp.append(val)\n",
    "        subsets_unique[key] = val\n",
    "\n",
    "for key, val in subsets_unique.items():        \n",
    "    #Add the feature subset to Candidemia, BDG and PCT in both sets:\n",
    "    sub2_BDG_PCT_y_training = training_BDG_PCT_std[['CANDIDEMIA','B_D_GLUCANO','PROCALCITONINA']+val]\n",
    "    sub2_BDG_PCT_y_test = test_BDG_PCT_std[['CANDIDEMIA','B_D_GLUCANO','PROCALCITONINA']+val]\n",
    "\n",
    "    #Splitting features and outcome:\n",
    "    X_training = sub2_BDG_PCT_y_training.iloc[:,1:sub2_BDG_PCT_y_training.shape[1]].values\n",
    "    y_training = sub2_BDG_PCT_y_training.iloc[:,0].values\n",
    "    y_training = np.double(y_training)\n",
    "    X_test = sub2_BDG_PCT_y_test.iloc[:,1:sub2_BDG_PCT_y_test.shape[1]].values\n",
    "    y_test = sub2_BDG_PCT_y_test.iloc[:,0].values\n",
    "    y_test = np.double(y_test)\n",
    "\n",
    "    #Range of threshold to evaluate:\n",
    "    thresholds = np.linspace(0.15,0.20,11) \n",
    "\n",
    "    #Classifiers to train:\n",
    "    classifiers = ['RF'] #LogisticRegL1 #LogisticRegL2\n",
    "    metrics_cl = {}\n",
    "\n",
    "    for cl in classifiers:\n",
    "        p = {}\n",
    "        p['model_name'] = cl\n",
    "        p = initiate_p(p)\n",
    "        print('Model name:', p['model_name'])\n",
    "\n",
    "        metrics_th = {}\n",
    "\n",
    "        for th in tqdm(thresholds):\n",
    "            p['threshold'] = th\n",
    "            print('Threshold:', p['threshold'])\n",
    "\n",
    "            performance = {}\n",
    "            performance['features'] = sub2_BDG_PCT_y_training.columns\n",
    "            #---------------------------------------------\n",
    "            #Hyperparameters search and model fit on training set:    \n",
    "            if p['model_name'] == 'LogisticRegL1':\n",
    "                score = make_scorer(my_tss_score,threshold=th,needs_proba=True)\n",
    "                fit = LogisticRegressionCV(cv = 10,random_state = 100,penalty='l1',solver='liblinear',scoring=score).fit(X_training, y_training) \n",
    "\n",
    "            elif p['model_name'] == 'LogisticRegL2':\n",
    "                score = make_scorer(my_tss_score,threshold=th,needs_proba=True)\n",
    "                fit = LogisticRegressionCV(cv = 10,random_state = 100,penalty='l2',scoring=score).fit(X_training, y_training) \n",
    "\n",
    "            elif p['model_name'] == 'RF':\n",
    "                p['grid'] = build_grid(p)\n",
    "                p['GS_RF'] = GridSearch_(X_training, y_training, p)\n",
    "                max_depth = p['GS_RF'].best_params_['max_depth']\n",
    "                n_estimators = p['GS_RF'].best_params_['n_estimators']\n",
    "                criterion = p['GS_RF'].best_params_['criterion']\n",
    "                max_features = p['GS_RF'].best_params_['max_features']\n",
    "                print(p['GS_RF'].best_params_)\n",
    "                performance['best_hyper'] = p['GS_RF'].best_params_\n",
    "                fit = RandomForestClassifier(max_depth=max_depth,n_estimators=n_estimators,\n",
    "                                             criterion=criterion,max_features=max_features,random_state=100).fit(X_training, y_training) \n",
    "\n",
    "            #other models can be added here\n",
    "            \n",
    "            #---------------------------------------------\n",
    "            #Predict on training set:\n",
    "            PRED_prob_tr = fit.predict_proba(X_training)\n",
    "            PRED_tr_yes = PRED_prob_tr[:,1] \n",
    "\n",
    "            PRED_tr_bin = PRED_tr_yes > th\n",
    "            PRED_tr_bin = PRED_tr_bin*1.\n",
    "\n",
    "            performance['ytr'] = y_training\n",
    "            performance['PRED_prob_tr'] = PRED_prob_tr\n",
    "\n",
    "            #Compute evaluation metrics on training set:\n",
    "            print(confusion_matrix(y_training,PRED_tr_bin))\n",
    "            tn_tr, fp_tr, fn_tr, tp_tr = confusion_matrix(y_training, PRED_tr_bin).ravel()\n",
    "            spec_tr = tn_tr / (tn_tr+fp_tr)\n",
    "            f1_tr = f1_score(y_training, PRED_tr_bin,average = 'weighted')\n",
    "            acc_tr = accuracy_score(y_training, PRED_tr_bin)\n",
    "            prec_tr = precision_score(y_training, PRED_tr_bin,average = 'weighted')\n",
    "            recall_tr = recall_score(y_training, PRED_tr_bin) \n",
    "            npv_tr = tn_tr / (tn_tr+fn_tr)\n",
    "\n",
    "            performance['tss_tr'] = recall_tr+spec_tr-1\n",
    "            performance['f1score_tr'] = f1_tr\n",
    "            performance['accuracy_tr'] = acc_tr\n",
    "            performance['precision_tr'] = prec_tr\n",
    "            performance['recall_tr'] = recall_tr\n",
    "            performance['specificity_tr'] = spec_tr \n",
    "            performance['npv_tr'] = npv_tr \n",
    "\n",
    "            #---------------------------------------------\n",
    "            #Predict on test set:\n",
    "            PRED_prob_ts = fit.predict_proba(X_test)\n",
    "            PRED_ts_yes = PRED_prob_ts[:,1]\n",
    "\n",
    "            PRED_ts_bin = PRED_ts_yes>th\n",
    "            PRED_ts_bin = PRED_ts_bin*1.\n",
    "\n",
    "            performance['yts'] = y_test\n",
    "            performance['PRED_prob_ts'] = PRED_prob_ts\n",
    "\n",
    "            #Compute evaluation metrics on test set:\n",
    "            print(confusion_matrix(y_test,PRED_ts_bin))\n",
    "            tn_ts, fp_ts, fn_ts, tp_ts = confusion_matrix(y_test,PRED_ts_bin).ravel()\n",
    "            spec_ts = tn_ts / (tn_ts+fp_ts)\n",
    "            f1_ts = f1_score(y_test, PRED_ts_bin,average = 'weighted')\n",
    "            acc_ts = accuracy_score(y_test, PRED_ts_bin)\n",
    "            prec_ts = precision_score(y_test, PRED_ts_bin,average = 'weighted')\n",
    "            recall_ts = recall_score(y_test, PRED_ts_bin)\n",
    "            npv_ts = tn_ts / (tn_ts+fn_ts)\n",
    "\n",
    "            performance['tss_ts'] = recall_ts+spec_ts-1\n",
    "            performance['f1score_ts'] = f1_ts\n",
    "            performance['accuracy_ts'] = acc_ts\n",
    "            performance['precision_ts'] = prec_ts\n",
    "            performance['recall_ts'] = recall_ts\n",
    "            performance['specificity_ts'] = spec_ts\n",
    "            performance['npv_ts'] = npv_ts \n",
    "            #---------------------------------------------\n",
    "            metrics_th[th] = performance\n",
    "        #---------------------------------------------\n",
    "        #Performances at each treshold:\n",
    "        metrics_cl[cl] = metrics_th\n",
    "        #Save on file:\n",
    "        #nome_file = '<insert your path>/Zoom/Cutoff' + str(N) + '_' + cl + '_Performances_rank'+ str(key)+'.npy' \n",
    "        #np.save(nome_file, metrics_th)\n",
    "    #Save on file:\n",
    "    #nome_file = '<insert your path>/Zoom/Complete_Performances_Cutoff_' + str(N)+'.npy'\n",
    "    #np.save(nome_file, metrics_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469d2c19-c810-47ff-96b9-316864fb54e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your path here:\n",
    "all_ranks = np.load('<insert your path>/all_ranks.npy', allow_pickle = True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89385d0-0de5-4991-87c7-59e4429accab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performances visualization (TSS, accuracy, F1-score, precision, NPV) on training set (Figure S5):\n",
    "N = [30,23,22,21,20,19,18,16,15,13,12,10,9,8,6,0] #key of subset_unique\n",
    "thresholds = np.linspace(0.15,0.20,11)\n",
    "\n",
    "for th in thresholds:\n",
    "    n = [] #number of features plus BDG and PCT\n",
    "    tss = []\n",
    "    f1 = []\n",
    "    acc = []\n",
    "    prec = []\n",
    "    se = []\n",
    "    sp = []\n",
    "    npv = []\n",
    "    \n",
    "    for i in N:\n",
    "        #insert your path here:\n",
    "        nome_file = '<insert your path>/Zoom/Cutoff18_RF_Performances_rank' + str(i) +'.npy'\n",
    "        file = np.load(nome_file, allow_pickle = True).item()\n",
    "        metrics = file[th]\n",
    "        \n",
    "        n.append(len(metrics['features'])-3)\n",
    "        tss.append(metrics['tss_tr'])\n",
    "        f1.append(metrics['f1score_tr'])\n",
    "        acc.append(metrics['accuracy_tr'])\n",
    "        prec.append(metrics['precision_tr'])\n",
    "        npv.append(metrics['npv_tr'])\n",
    "        se.append(metrics['recall_tr'])\n",
    "        sp.append(metrics['specificity_tr'])\n",
    "        \n",
    "        #Likelihood Ratios:\n",
    "        #pos_LR_tr, neg_LR_tr = class_likelihood_ratios(metrics['ytr'],PRED_tr_bin)\n",
    "        #lr_p.append(pos_LR_tr)\n",
    "        #lr_n.append(neg_LR_tr)\n",
    "        \n",
    "    print(tss)\n",
    "        \n",
    "    #performances to visualize:\n",
    "    y = np.array([tss,f1,acc,prec,npv]).transpose() #,lr_p,lr_n\n",
    "    plt.xlabel(\"N\")\n",
    "    plt.ylabel(\"Model performance\")\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.title(str(th))\n",
    "\n",
    "    legend = ['TSS', 'F1-score', 'Accuracy', 'Precision','NPV'] #, \"LR+\",\"LR-\"\n",
    "    colors = ['#fdb462','#b3de69','#bebada','#fb8072','#80b1d3'] #,'#8dd3c7','#ffffb3'\n",
    "\n",
    "    for i in range(len(y[0])):\n",
    "        plt.plot(n,[pt[i] for pt in y],label = legend[i], color = colors[i])\n",
    "    plt.legend(loc=(0.82,0.22),fontsize='x-small')\n",
    "    plt.xticks(n,size=6.5)\n",
    "    plt.yticks(size=6.5)\n",
    "    #nome_figura = '<insert your path>/Zoom/Model_tss_f1_acc_prec_trend_npv_' + str(th) +'.png'\n",
    "    #plt.savefig(nome_figura,dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2efcfd-7071-4109-b8e7-e6a29ca0fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performances visualization (recall, SP) on training set (Figure S5):\n",
    "N = [30,23,22,21,20,19,18,16,15,13,12,10,9,8,6,0] #key of subset_unique\n",
    "thresholds = np.linspace(0.15,0.20,11)\n",
    "\n",
    "for th in thresholds:\n",
    "    n = [] #number of features plus BDG and PCT\n",
    "    tss = []\n",
    "    f1 = []\n",
    "    acc = []\n",
    "    prec = []\n",
    "    se = []\n",
    "    sp = []\n",
    "    \n",
    "    for i in N:\n",
    "        #insert your path here:\n",
    "        nome_file = '<insert your path>/Zoom/Cutoff18_RF_Performances_rank' + str(i) +'.npy'\n",
    "        file = np.load(nome_file, allow_pickle = True).item()\n",
    "        metrics = file[th]\n",
    "        \n",
    "        n.append(len(metrics['features'])-3)\n",
    "        tss.append(metrics['tss_tr'])\n",
    "        f1.append(metrics['f1score_tr'])\n",
    "        acc.append(metrics['accuracy_tr'])\n",
    "        prec.append(metrics['precision_tr'])\n",
    "        npv.append(metrics['npv_tr'])\n",
    "        se.append(metrics['recall_tr'])\n",
    "        sp.append(metrics['specificity_tr'])\n",
    "        \n",
    "        #Likelihood Ratios:\n",
    "        #pos_LR_tr, neg_LR_tr = class_likelihood_ratios(metrics['ytr'],PRED_tr_bin)\n",
    "        #lr_p.append(pos_LR_tr)\n",
    "        #lr_n.append(neg_LR_tr)\n",
    "        \n",
    "    #performances to visualize:\n",
    "    y = np.array([se,sp]).transpose() \n",
    "    plt.xlabel(\"N\")\n",
    "    plt.ylabel(\"Model performance\")\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    #plt.title(str(th))\n",
    "\n",
    "    legend = ['Recall', 'Specificity']\n",
    "    colors = ['#8dd3c7','#fff033']\n",
    "\n",
    "    for i in range(len(y[0])):\n",
    "        plt.plot(n,[pt[i] for pt in y],label = legend[i], color = colors[i])\n",
    "    plt.legend(loc=(0.82,0.22),fontsize='x-small')\n",
    "    plt.xticks(n,size=6.5)\n",
    "    plt.yticks(size=6.5)\n",
    "    plt.axhline(y=0.6, color='black', linestyle='-.',linewidth=0.7)\n",
    "    #nome_figura = '<insert your path>/Zoom/Model_se_sp_trend_' + str(th) +'.png'\n",
    "    #plt.savefig(nome_figura,dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272ac72-82b7-4fcd-90b9-f88d7bbddcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison of performances' mean and standard deviation for subsets with 9<=N<=26, given each threshold between 0.15 and 0.18 (Table S5):\n",
    "N = [30,23,22,21,20,19,18,16,15,13,12,10,9,8,6,0] #key of subset_unique\n",
    "thresholds = np.linspace(0.15,0.20,11)\n",
    "\n",
    "new_thresholds_training = {}\n",
    "new_thresholds_test = {}\n",
    "\n",
    "for th in thresholds:\n",
    "    if th >=0.15 and th <=0.18:\n",
    "        \n",
    "        new_thresholds_training[th] = {}\n",
    "        new_thresholds_test[th] = {}\n",
    "        \n",
    "        tss_tr,f1_tr,accuracy_tr,precision_tr,recall_tr,specificity_tr,npv_tr,lr_pos_tr,lr_neg_tr = [],[],[],[],[],[],[],[],[]\n",
    "        tss_ts,f1_ts,accuracy_ts,precision_ts,recall_ts,specificity_ts,npv_ts,lr_pos_ts,lr_neg_ts = [],[],[],[],[],[],[],[],[]\n",
    "        \n",
    "        for i in N:\n",
    "            #isert your path here:\n",
    "            nome_file = '<insert your path>/Zoom/Cutoff18_RF_Performances_rank' + str(i) +'.npy'\n",
    "            file = np.load(nome_file, allow_pickle = True).item()\n",
    "            metrics = file[th]\n",
    "            num_feat = len(metrics['features'])-3\n",
    "            \n",
    "\n",
    "            if num_feat >= 9 and num_feat <= 26:\n",
    "                tss_tr.append(metrics['tss_tr'])\n",
    "                f1_tr.append(metrics['f1score_tr'])\n",
    "                accuracy_tr.append(metrics['accuracy_tr'])\n",
    "                precision_tr.append(metrics['precision_tr'])\n",
    "                npv_tr.append(metrics['npv_tr'])\n",
    "                recall_tr.append(metrics['recall_tr'])\n",
    "                specificity_tr.append(metrics['specificity_tr'])\n",
    "                \n",
    "                pos_LR_tr, neg_LR_tr = class_likelihood_ratios(metrics['ytr'],PRED_tr_bin)\n",
    "                lr_pos_tr.append(pos_LR_tr)\n",
    "                lr_neg_tr.append(neg_LR_tr)\n",
    "                \n",
    "                tss_ts.append(metrics['tss_ts'])\n",
    "                f1_ts.append(metrics['f1score_ts'])\n",
    "                accuracy_ts.append(metrics['accuracy_ts'])\n",
    "                precision_ts.append(metrics['precision_ts'])\n",
    "                npv_ts.append(metrics['npv_ts'])\n",
    "                recall_ts.append(metrics['recall_ts'])\n",
    "                specificity_ts.append(metrics['specificity_ts'])\n",
    "                \n",
    "                pos_LR_ts, neg_LR_ts = class_likelihood_ratios(metrics['yts'],PRED_ts_bin)\n",
    "                lr_pos_ts.append(pos_LR_ts)\n",
    "                lr_neg_ts.append(neg_LR_ts)\n",
    "                \n",
    "            new_thresholds_training[th]['TSS'] = tss_tr\n",
    "            new_thresholds_training[th]['F1'] = f1_tr\n",
    "            new_thresholds_training[th]['ACC'] = accuracy_tr\n",
    "            new_thresholds_training[th]['PREC'] = precision_tr\n",
    "            new_thresholds_training[th]['SE'] = recall_tr\n",
    "            new_thresholds_training[th]['SP'] = specificity_tr\n",
    "            new_thresholds_training[th]['NPV'] = npv_tr\n",
    "            new_thresholds_training[th]['LR+'] = lr_pos_tr\n",
    "            new_thresholds_training[th]['LR-'] = lr_neg_tr\n",
    "            \n",
    "            \n",
    "            new_thresholds_test[th]['TSS'] = tss_ts\n",
    "            new_thresholds_test[th]['F1'] = f1_ts\n",
    "            new_thresholds_test[th]['ACC'] = accuracy_ts\n",
    "            new_thresholds_test[th]['PREC'] = precision_ts\n",
    "            new_thresholds_test[th]['SE'] = recall_ts\n",
    "            new_thresholds_test[th]['SP'] = specificity_ts\n",
    "            new_thresholds_test[th]['NPV'] = npv_ts\n",
    "            new_thresholds_test[th]['LR+'] = lr_pos_ts\n",
    "            new_thresholds_test[th]['LR-'] = lr_neg_ts\n",
    "\n",
    "print('On training set:')\n",
    "for k in list(new_thresholds_training.keys()):\n",
    "    print('Threshold' ,k)\n",
    "    print('TSS mean value and standard deviation are: ', np.mean(new_thresholds_training[k]['TSS']),',' ,np.std(new_thresholds_training[k]['TSS']))\n",
    "    print('F1-score mean value and standard deviation are: ', np.mean(new_thresholds_training[k]['F1']),',' ,np.std(new_thresholds_training[k]['F1']))\n",
    "    print('Accuracy mean value and standard deviation are: ', np.mean(new_thresholds_training[k]['ACC']),',' ,np.std(new_thresholds_training[k]['ACC']))\n",
    "    print('Precision mean value and standard deviation are: ', np.mean(new_thresholds_training[k]['PREC']),',' ,np.std(new_thresholds_training[k]['PREC']))\n",
    "    print('Recall mean value and standard deviation are: ', np.mean(new_thresholds_training[k]['SE']),',' ,np.std(new_thresholds_training[k]['SE']))\n",
    "    print('Specificity mean value and standard deviation are: ', np.mean(new_thresholds_training[k]['SP']),',' ,np.std(new_thresholds_training[k]['SP']))\n",
    "    print('NPV mean value and standard deviation are: ', np.mean(new_thresholds_training[k]['NPV']),',' ,np.std(new_thresholds_training[k]['NPV']))\n",
    "    print('LR+ mean value and standard deviation are: ', np.mean(new_thresholds_training[k]['LR+']),',' ,np.std(new_thresholds_training[k]['LR+']))\n",
    "    print('LR- mean value and standard deviation are: ', np.mean(new_thresholds_training[k]['LR-']),',' ,np.std(new_thresholds_training[k]['LR-']))\n",
    "print('\\n')   \n",
    "print('On test set:')\n",
    "for k in list(new_thresholds_test.keys()):\n",
    "    print('Threshold' ,k)\n",
    "    print('TSS mean value and standard deviation are: ', np.mean(new_thresholds_test[k]['TSS']),',' ,np.std(new_thresholds_test[k]['TSS']))\n",
    "    print('F1-score mean value and standard deviation are: ', np.mean(new_thresholds_test[k]['F1']),',' ,np.std(new_thresholds_test[k]['F1']))\n",
    "    print('Accuracy mean value and standard deviation are: ', np.mean(new_thresholds_test[k]['ACC']),',' ,np.std(new_thresholds_test[k]['ACC']))\n",
    "    print('Precision mean value and standard deviation are: ', np.mean(new_thresholds_test[k]['PREC']),',' ,np.std(new_thresholds_test[k]['PREC']))\n",
    "    print('Recall mean value and standard deviation are: ', np.mean(new_thresholds_test[k]['SE']),',' ,np.std(new_thresholds_test[k]['SE']))\n",
    "    print('Specificity mean value and standard deviation are: ', np.mean(new_thresholds_test[k]['SP']),',' ,np.std(new_thresholds_test[k]['SP']))\n",
    "    print('NPV mean value and standard deviation are: ', np.mean(new_thresholds_test[k]['NPV']),',' ,np.std(new_thresholds_test[k]['NPV']))\n",
    "    print('LR+ mean value and standard deviation are: ', np.mean(new_thresholds_test[k]['LR+']),',' ,np.std(new_thresholds_test[k]['LR+']))\n",
    "    print('LR- mean value and standard deviation are: ', np.mean(new_thresholds_test[k]['LR-']),',' ,np.std(new_thresholds_test[k]['LR-']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c119f8d-4706-44ea-a873-d7f852f0b920",
   "metadata": {},
   "source": [
    "###### <font color='indianred'><b>CHOICE 1: Threshold = 0.175</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c06c3-4a2b-4024-801f-5123c96edf0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### <font color='indianred'><b>CHOICE 2: Subset with 12 features plus BDG and PCT</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a3621-69f2-4865-a0ab-f3c970655835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No other data set needs to be exported for following analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
