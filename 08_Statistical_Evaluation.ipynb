{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a5ed043-1340-4b23-9aec-a6d4a42460a1",
   "metadata": {},
   "source": [
    "# <font color='Crimson'><b>STATISTICAL EVALUATION</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2394b46-6be0-4307-bd1e-1b1bcddadbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages:\n",
    ".\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import make_scorer\n",
    "#from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import statistics\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack\n",
    "from scipy.stats import ranksums,wilcoxon\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import class_likelihood_ratios\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b7c38-d9ca-46cc-8ed5-d481b8431ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define useful functions for classification task:\n",
    "#TSS score\n",
    "def my_tss_score(Y_training, probability_prediction,threshold):\n",
    "    \n",
    "    Y_predicted = probability_prediction > threshold\n",
    "    res = metrics_classification(Y_training > 0, Y_predicted, print_skills=False)    \n",
    "\n",
    "    return res['tss']\n",
    "\n",
    "#Classification metrics\n",
    "def metrics_classification(y_real, y_pred, print_skills=True):\n",
    "\n",
    "    cm, far, pod, acc, hss, tss, fnfp, csi, tpr, tnr = classification_skills(y_real, y_pred)\n",
    "\n",
    "    if print_skills:\n",
    "        print ('confusion matrix')\n",
    "        print (cm)\n",
    "        print ('false alarm ratio       \\t', far)\n",
    "        print ('probability of detection\\t', pod)\n",
    "        print ('accuracy                \\t', acc)\n",
    "        print ('hss                     \\t', hss)\n",
    "        print ('tss                     \\t', tss)\n",
    "        print ('balance                 \\t', fnfp)\n",
    "        print ('csi                 \\t', csi)\n",
    "        print ('tpr                 \\t', tpr)\n",
    "        print ('tnr                 \\t', tnr)\n",
    "\n",
    "    balance_label = float(sum(y_real)) / y_real.shape[0]\n",
    "\n",
    "    return {\n",
    "        \"cm\": cm,\n",
    "        \"far\": far,\n",
    "        \"pod\": pod,\n",
    "        \"acc\": acc,\n",
    "        \"hss\": hss,\n",
    "        \"tss\": tss,\n",
    "        \"fnfp\": fnfp,\n",
    "        \"balance label\": balance_label,\n",
    "        \"csi\": csi,\n",
    "        \"tpr\": tpr,\n",
    "        \"tnr\": tnr}\n",
    "\n",
    "def classification_skills(y_real, y_pred):\n",
    "\n",
    "    cm = confusion_matrix(y_real, y_pred)\n",
    "\n",
    "    if cm.shape[0] == 1 and sum(y_real) == 0:\n",
    "        a = 0.\n",
    "        d = float(cm[0, 0])\n",
    "        b = 0.\n",
    "        c = 0.\n",
    "    elif cm.shape[0] == 1 and sum(y_real) == y_real.shape[0]:\n",
    "        a = float(cm[0, 0])\n",
    "        d = 0.\n",
    "        b = 0.\n",
    "        c = 0.\n",
    "    elif cm.shape[0] == 2:\n",
    "        a = float(cm[1, 1])\n",
    "        d = float(cm[0, 0])\n",
    "        b = float(cm[0, 1])\n",
    "        c = float(cm[1, 0])\n",
    "    TP = a\n",
    "    TN = d\n",
    "    FP = b\n",
    "    FN = c\n",
    "\n",
    "    if (TP + FP + FN + TN) == 0.:\n",
    "        if (TP + TN) == 0.:\n",
    "            acc = 0.  # float('NaN')\n",
    "        else:\n",
    "            acc = -100  # float('Inf')\n",
    "    else:\n",
    "        acc = (TP + TN) / (TP + FP + FN + TN)\n",
    "        \n",
    "\n",
    "    if TP + FN == 0.:\n",
    "        if TP == 0.:\n",
    "            tss_aux1 = 0.  # float('NaN')\n",
    "        else:\n",
    "            tss_aux1 = -100  # float('Inf')\n",
    "    else:\n",
    "        tss_aux1 = (TP / (TP + FN))\n",
    "\n",
    "    if (FP + TN) == 0.:\n",
    "        if FP == 0.:\n",
    "            tss_aux2 = 0.  # float('NaN')\n",
    "        else:\n",
    "            tss_aux2 = -100  # float('Inf')\n",
    "    else:\n",
    "        tss_aux2 = (FP / (FP + TN))\n",
    "\n",
    "    tss = tss_aux1 - tss_aux2\n",
    "\n",
    "    if ((TP + FN) * (FN + TN) + (TP + FP) * (FP + TN)) == 0.:\n",
    "        if (TP * TN - FN * FP) == 0:\n",
    "            hss = 0.  # float('NaN')\n",
    "        else:\n",
    "            hss = -100  # float('Inf')\n",
    "    else:\n",
    "        hss = 2 * (TP * TN - FN * FP) / ((TP + FN) *\n",
    "                                         (FN + TN) + (TP + FP) * (FP + TN))\n",
    "\n",
    "    if FP == 0.:\n",
    "        if FN == 0.:\n",
    "            fnfp = 0.  # float('NaN')\n",
    "        else:\n",
    "            fnfp = -100  # float('Inf')\n",
    "    else:\n",
    "        fnfp = FN / FP\n",
    "\n",
    "    if (TP + FN) == 0.:\n",
    "        if TP == 0.:\n",
    "            pod = 0  # float('NaN')\n",
    "        else:\n",
    "            pod = -100  # float('Inf')\n",
    "    else:\n",
    "        pod = TP / (TP + FN)\n",
    "\n",
    "\n",
    "    if (TP + FP) == 0.:\n",
    "        if FP == 0.:\n",
    "            far = 0.  # float('NaN')\n",
    "        else:\n",
    "            far = -100  # float('Inf')\n",
    "    else:\n",
    "        far = FP / (TP + FP)\n",
    "\n",
    "    #acc = (a + d) / (a + b + c + d)\n",
    "    tpr = tss_aux1  # a / (a + b)\n",
    "    tnr = 1-tss_aux2  # d / (d + c)\n",
    "    #wtpr = a / (a + b) * (a + c) / (a + b + c + d) + d / (c + d) * (b + d) / (a + b + c + d)\n",
    "    #pacc = a / (a + c)\n",
    "    #nacc = d / (b + d)\n",
    "    #wacc = a / (a + c) * (a + c) / (a + b + c + d) + d / (b + d) * (b + d) / (a + b + c + d)\n",
    "\n",
    "    # if the cm has a row or a column equal to 0, we have bad tss\n",
    "    if TP+FN == 0 or TN+FP == 0 or TP+FP == 0 or TN+FN == 0:\n",
    "        tss = 0\n",
    "    if TP+FP+FN==0:\n",
    "        csi = 0\n",
    "    else:\n",
    "        csi = TP/(TP+FP+FN)\n",
    "\n",
    "    return cm.tolist(), far, pod, acc, hss, tss, fnfp, csi, tpr, tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dfdfbc-7ac8-4233-b63b-aef0d5a9c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define useful fuctions for cross-validation:\n",
    "def build_grid(p):\n",
    "    #Parameters initialization:\n",
    "    model_name = None if p['model_name'] == None else p['model_name']\n",
    "    GRID = None \n",
    "\n",
    "    if model_name == \"RF\":\n",
    "        GRID = {'n_estimators': p['n_estimators'],\n",
    "                'max_features': p['max_features'],\n",
    "                'max_depth': p['max_depth'],\n",
    "                'criterion': p['criterion']\n",
    "               }   \n",
    "\n",
    "    #other models can be added here\n",
    "\n",
    "    return GRID\n",
    "\n",
    "\n",
    "def initiate_p(p):\n",
    "    #Ranges for hyperparameter search:\n",
    "    if p['model_name'] == 'RF':\n",
    "        p['estimator'] = RandomForestClassifier(random_state=100)\n",
    "        p['n_estimators'] = [100,200,300,400,500] \n",
    "        p['max_features'] = [None, 'sqrt', 'log2']\n",
    "        p['max_depth'] = [4,5,6,7,8]\n",
    "        p['criterion'] = ['gini','entropy']\n",
    "\n",
    "    #other models can be added here\n",
    "\n",
    "    #All:\n",
    "    p['cv'] = 10\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def GridSearch_(X,y,p):\n",
    "    #Parameters initialization:\n",
    "    estimator = None if p['estimator'] == None else p['estimator']\n",
    "    cv = 10 if p['cv'] == None else p['cv']\n",
    "    grid = None if p['grid'] == None else p['grid']\n",
    "    tau = 0.5 if p['threshold'] == None else p['threshold']\n",
    "    \n",
    "    if(estimator == None or grid == None):\n",
    "        return None\n",
    "\n",
    "    #1st step: select the best hyperparameters\n",
    "    CV = GridSearchCV(estimator     = estimator,\n",
    "                        param_grid  = grid,\n",
    "                        scoring     =  make_scorer(my_tss_score,threshold=tau,needs_proba=True), #TSS score\n",
    "                        #refit       = 'roc_auc',\n",
    "                        cv          = cv,\n",
    "                        verbose     = 0,\n",
    "                        n_jobs      = 20,\n",
    "                        return_train_score=True)\n",
    "    CV_H = CV.fit(X,y)\n",
    "    \n",
    "    return CV_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440db7c2-2ee6-48eb-8d02-6f10ee67c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset:\n",
    "df_complete1 = pd.read_csv('02_1_Dataset.csv', sep=',',index_col=0)\n",
    "df_complete1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a4250-ab1d-4090-91e9-7b931dfcfb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select BDG and PCT columns:\n",
    "df_complete1_BDG_PCT = df_complete1.iloc[:,[40,42]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2123e-90e2-4a27-b2d1-3cfcda728d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset:\n",
    "df_dataset = pd.read_csv('02_2_Dataset.csv', sep=',',index_col=0)\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e44edb-1db6-4f64-8a9f-ded698b861c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the last imported dataset and BDG and PCT columns:\n",
    "df_dataset2 = df_dataset.drop(df_dataset.columns[[0,3,4,5,9]], axis=1)\n",
    "df_complete2_BDG_PCT = df_complete1_BDG_PCT.merge(df_dataset2, how='right',right_index=True,left_index=True)\n",
    "df_complete2_BDG_PCT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0099b-6162-45cf-98e8-03395a9f7a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete episodes with missing BDG or PCT:\n",
    "df_BDG_PCT = df_complete2_BDG_PCT[~(df_complete2_BDG_PCT['PROCALCITONINA'].isna() | df_complete2_BDG_PCT['B_D_GLUCANO'].isna())]\n",
    "print('Number of samples in the new dataset:',df_BDG_PCT.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae587209-1519-40b4-a345-f2be406519df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation and Standardization: within stratified 10-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad195021-43b9-45e0-9367-8ee1a44c52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New percentage of candidemias:\n",
    "print('Number of candidemias in the new dataset:', pd.value_counts(df_BDG_PCT['CANDIDEMIA'])[1], \n",
    "      '(',round(pd.value_counts(df_BDG_PCT['CANDIDEMIA'], normalize=True)[1]*100,2),'%) of',\n",
    "      df_BDG_PCT['CANDIDEMIA'].shape[0], 'samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe98e0-adf6-4e3d-8fa6-f317c985c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 subset of interest to compare statistically: BDG and PCT only, all features, best subset chosen with and without BDG and PCT:\n",
    "#Train best classifier with StratifiedKFold (for more than one shuffle) to evaluate performances (on validation sets) consistently:\n",
    "\n",
    "#Define the number of shuffles:\n",
    "R = 3\n",
    "#Define the number of folds:\n",
    "K = 10\n",
    "#Threshold chosen:\n",
    "thresholds = np.linspace(0.175,0.175,1)\n",
    "#Classifier chosen:\n",
    "classifiers = ['RF'] #LogisticRegL1 #LogisticRegL2\n",
    "\n",
    "#Subsets to evaluate:\n",
    "dataset = ['All','BDG_PCT','with_12','without_12']\n",
    "\n",
    "for d in dataset:\n",
    "    \n",
    "    if d == 'BDG_PCT':\n",
    "        subset_training = df_BDG_PCT[['CANDIDEMIA', 'B_D_GLUCANO', 'PROCALCITONINA']]\n",
    "    elif d == 'All':\n",
    "        subset_training = df_BDG_PCT.drop('MISTA',axis=1)\n",
    "    elif d == 'with_12':\n",
    "        subset_training = df_BDG_PCT[['CANDIDEMIA', 'B_D_GLUCANO', 'PROCALCITONINA', 'EOSINOFILI', 'LINFOCITI', 'MONOCITI', 'NEUTROFILI', \n",
    "                                      'EMATOCRITO', 'EMOGLOBINA', 'GLOBULI_B', 'PIASTRINE', 'TEMPO_PROTROMB', 'ACIDO_URICO', 'UREA', 'ALBUMINA']]\n",
    "    elif d == 'without_12':\n",
    "        subset_training = df_BDG_PCT[['CANDIDEMIA', 'EOSINOFILI', 'LINFOCITI', 'MONOCITI', 'NEUTROFILI', \n",
    "                                      'EMATOCRITO', 'EMOGLOBINA', 'GLOBULI_B', 'PIASTRINE', 'TEMPO_PROTROMB', 'ACIDO_URICO', 'UREA', 'ALBUMINA']]\n",
    "        \n",
    "    #Splitting features and outcome:\n",
    "    X_training = subset_training.drop('CANDIDEMIA',axis=1) #.values\n",
    "    y_training = subset_training[['CANDIDEMIA']] #.values\n",
    "    #y_training = np.double(y_training)\n",
    "    \n",
    "    #Shuffle data in order to test stability of the model:\n",
    "    #In order to guarantee reproducibility of results, the random states are always the numbers 0,1,2.\n",
    "    for r in range(0,R):\n",
    "        X,y = shuffle(X_training,y_training,random_state = r) \n",
    "        #------------------------------------------------------------------------------------------\n",
    "        # CROSS-VALIDATION: K-Fold\n",
    "        kf = StratifiedKFold(n_splits=K, shuffle=False)\n",
    "        #------------------------------------------------------------------------------------------\n",
    "        Kfold = 0\n",
    "\n",
    "        for train_index, validation_index in kf.split(X,y):\n",
    "\n",
    "            X_training, X_validation = X.iloc[train_index,:], X.iloc[validation_index,:]\n",
    "            y_training0, y_validation0 = y.iloc[train_index], y.iloc[validation_index]\n",
    "            \n",
    "            train = pd.concat([X_training,y_training0],axis=1)\n",
    "            validation = pd.concat([X_validation,y_validation0],axis=1)\n",
    "            \n",
    "            #Imputation\n",
    "            #Definition of the imputer:\n",
    "            imputer = IterativeImputer(KNeighborsRegressor(n_neighbors=5),\n",
    "                                       sample_posterior=False, \n",
    "                                       max_iter=100,\n",
    "                                       tol=0.05,\n",
    "                                       n_nearest_features=None,\n",
    "                                       initial_strategy='most_frequent',\n",
    "                                       imputation_order='random',\n",
    "                                       random_state=100)\n",
    "            #Fit of the imputer on training set and imputation:\n",
    "            train_imputed = imputer.fit_transform(train)\n",
    "            train_imputed = pd.DataFrame(train_imputed)\n",
    "            #Work on imputed training set:\n",
    "            train_imputed.columns = train.columns\n",
    "            if d=='All':\n",
    "                train_imputed['30gg'] = train_imputed['30gg'] > 0.5\n",
    "                train_imputed['30gg'] = train_imputed['30gg']*1.\n",
    "            #Imputation of the validation set:\n",
    "            validation_imputed = imputer.transform(validation)\n",
    "            validation_imputed = pd.DataFrame(validation_imputed)\n",
    "            #Work on imputed test set:\n",
    "            validation_imputed.columns = validation.columns\n",
    "            if d=='All':\n",
    "                validation_imputed['30gg'] = validation_imputed['30gg'] > 0.5\n",
    "                validation_imputed['30gg'] = validation_imputed['30gg']*1.\n",
    "            \n",
    "            #Standardization\n",
    "            #Define the scaler:\n",
    "            scaler = StandardScaler()\n",
    "            #Fit of the scaler on the continuous features of the training set and scale:\n",
    "            if d=='All':\n",
    "                X_train_std0 = pd.DataFrame(scaler.fit_transform(train_imputed.drop(['CANDIDEMIA','SESSO','30gg'],axis=1)))\n",
    "                X_train_std1 = pd.DataFrame(np.concatenate((train_imputed[['SESSO','30gg']], X_train_std0), axis=1))\n",
    "            else:\n",
    "                X_train_std1 = pd.DataFrame(scaler.fit_transform(train_imputed.drop('CANDIDEMIA',axis=1)))\n",
    "                \n",
    "            #Scale the continuous features of the test set:\n",
    "            if d=='All':\n",
    "                X_validation_std0 = pd.DataFrame(scaler.transform(validation_imputed.drop(['CANDIDEMIA','SESSO','30gg'],axis=1)))\n",
    "                X_validation_std1 = pd.DataFrame(np.concatenate((validation_imputed[['SESSO','30gg']], X_validation_std0), axis=1))\n",
    "            else:\n",
    "                X_validation_std1 = pd.DataFrame(scaler.transform(validation_imputed.drop('CANDIDEMIA',axis=1)))\n",
    "                \n",
    "            X_train_std = X_train_std1.values\n",
    "            X_validation_std = X_validation_std1.values\n",
    "            y_training = y_training0.values.ravel()\n",
    "            y_training = np.double(y_training)\n",
    "            y_validation = y_validation0.values.ravel()\n",
    "            y_validation = np.double(y_validation)\n",
    "\n",
    "            for cl in classifiers:\n",
    "                metrics_cl = {}\n",
    "\n",
    "                #metrics_cl['train_index'] = train_index\n",
    "                #metrics_cl['validation_index'] = validation_index\n",
    "\n",
    "                p = {}\n",
    "                p['model_name'] = cl\n",
    "                p = initiate_p(p)\n",
    "                print('Model name:', p['model_name'])\n",
    "\n",
    "                metrics_th = {}\n",
    "\n",
    "                for th in tqdm(thresholds):\n",
    "                    p['threshold'] = th\n",
    "                    print('Threshold:', p['threshold'])\n",
    "\n",
    "                    performance = {}\n",
    "                    #---------------------------------------------\n",
    "                    #Hyperparameters search and model fit on training set:    \n",
    "                    if p['model_name'] == 'LogisticRegL1':\n",
    "                        score = make_scorer(my_tss_score,threshold=th,needs_proba=True)\n",
    "                        fit = LogisticRegressionCV(cv = 10,random_state = 100,penalty='l1',solver='liblinear',\n",
    "                                                   scoring=score).fit(X_train_std, y_training) \n",
    "\n",
    "                    elif p['model_name'] == 'LogisticRegL2':\n",
    "                        score = make_scorer(my_tss_score,threshold=th,needs_proba=True)\n",
    "                        fit = LogisticRegressionCV(cv = 10,random_state = 100,penalty='l2',scoring=score).fit(X_train_std, y_training) \n",
    "\n",
    "                    if p['model_name'] == 'RF':\n",
    "                        p['grid'] = build_grid(p)\n",
    "                        p['GS_RF'] = GridSearch_(X_train_std, y_training, p)\n",
    "                        max_depth = p['GS_RF'].best_params_['max_depth']\n",
    "                        n_estimators = p['GS_RF'].best_params_['n_estimators']\n",
    "                        criterion = p['GS_RF'].best_params_['criterion']\n",
    "                        max_features = p['GS_RF'].best_params_['max_features']\n",
    "                        print(p['GS_RF'].best_params_)\n",
    "                        #performance['best_hyper'] = p['GS_RF'].best_params_\n",
    "                        fit = RandomForestClassifier(max_depth=max_depth,n_estimators=n_estimators,criterion=criterion,\n",
    "                                                     max_features=max_features,random_state=100).fit(X_train_std, y_training) \n",
    "\n",
    "                    #other models can be added here\n",
    "                    \n",
    "                    #---------------------------------------------\n",
    "                    #Predict on training set:\n",
    "                    PRED_prob_tr = fit.predict_proba(X_train_std)\n",
    "                    PRED_tr_yes = PRED_prob_tr[:,1] \n",
    "\n",
    "                    PRED_tr_bin = PRED_tr_yes > th\n",
    "                    PRED_tr_bin = PRED_tr_bin*1.\n",
    "\n",
    "                    performance['y_training'] = y_training\n",
    "                    performance['PRED_prob_tr'] = PRED_prob_tr\n",
    "\n",
    "                    #Compute evaluation metrics on training set:\n",
    "                    print(confusion_matrix(y_training,PRED_tr_bin))\n",
    "                    tn_tr, fp_tr, fn_tr, tp_tr = confusion_matrix(y_training, PRED_tr_bin).ravel()\n",
    "                    spec_tr = tn_tr / (tn_tr+fp_tr)\n",
    "                    f1_tr = f1_score(y_training, PRED_tr_bin,average = 'weighted')\n",
    "                    acc_tr = accuracy_score(y_training, PRED_tr_bin)\n",
    "                    prec_tr = precision_score(y_training, PRED_tr_bin,average = 'weighted')\n",
    "                    recall_tr = recall_score(y_training, PRED_tr_bin) \n",
    "                    npv_tr = tn_tr / (tn_tr+fn_tr)\n",
    "\n",
    "                    performance['tss_tr'] = recall_tr+spec_tr-1\n",
    "                    performance['f1score_tr'] = f1_tr\n",
    "                    performance['accuracy_tr'] = acc_tr\n",
    "                    performance['precision_tr'] = prec_tr\n",
    "                    performance['recall_tr'] = recall_tr\n",
    "                    performance['specificity_tr'] = spec_tr \n",
    "                    performance['npv_tr'] = npv_tr \n",
    "                    \n",
    "                    #---------------------------------------------\n",
    "                    #Predict on validation set:\n",
    "                    PRED_prob_ts = fit.predict_proba(X_validation_std)\n",
    "                    PRED_ts_yes = PRED_prob_ts[:,1]\n",
    "\n",
    "                    PRED_ts_bin = PRED_ts_yes>th\n",
    "                    PRED_ts_bin = PRED_ts_bin*1.\n",
    "\n",
    "                    performance['yts'] = y_validation\n",
    "                    performance['PRED_prob_ts'] = PRED_prob_ts\n",
    "\n",
    "                    #Compute evaluation metrics on validation set:\n",
    "                    print(confusion_matrix(y_validation,PRED_ts_bin))\n",
    "                    tn_ts, fp_ts, fn_ts, tp_ts = confusion_matrix(y_validation,PRED_ts_bin).ravel()\n",
    "                    spec_ts = tn_ts / (tn_ts+fp_ts)\n",
    "                    f1_ts = f1_score(y_validation, PRED_ts_bin,average = 'weighted')\n",
    "                    acc_ts = accuracy_score(y_validation, PRED_ts_bin)\n",
    "                    prec_ts = precision_score(y_validation, PRED_ts_bin,average = 'weighted')\n",
    "                    recall_ts = recall_score(y_validation, PRED_ts_bin)  \n",
    "                    npv_ts = tn_ts / (tn_ts+fn_ts)\n",
    "\n",
    "                    performance['tss_ts'] = recall_ts+spec_ts-1\n",
    "                    performance['f1score_ts'] = f1_ts\n",
    "                    performance['accuracy_ts'] = acc_ts\n",
    "                    performance['precision_ts'] = prec_ts\n",
    "                    performance['recall_ts'] = recall_ts\n",
    "                    performance['specificity_ts'] = spec_ts\n",
    "                    performance['npv_ts'] = npv_ts\n",
    "                    #---------------------------------------------\n",
    "                    metrics_th[th] = performance\n",
    "                #---------------------------------------------\n",
    "                #Performances at each treshold:\n",
    "                metrics_cl[cl] = metrics_th\n",
    "                #---------------------------------------------\n",
    "                # Save on file:                \n",
    "                #nome_file = \"<insert your path>/\" + d + \"/\" + d +\"_R\"+ str(r) + \"_K\" + str(Kfold) + '_' + p['model_name'] + '.npy'\n",
    "                #np.save(nome_file, metrics_cl)\n",
    "                Kfold += 1\n",
    "            #---------------------------------------------\n",
    "    p_shuffle = {}\n",
    "    p_shuffle['R'] = r\n",
    "    #------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4765fd-084e-4ab6-aa15-b4ffe9ab6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create one dictionary with vectors of performances for each subset: Key:Subset , Value: {Key:Measure of performance, Value:[30 values]}\n",
    "dataset_list = ['BDG_PCT','All','with_12','without_12']\n",
    "\n",
    "for d in dataset_list:\n",
    "    metrics = {}\n",
    "    nn = 0\n",
    "    #isert your path here:\n",
    "    file_name = '<insert your path>/' + d\n",
    "    for R in range(0,3):\n",
    "        for K in range(0,10):\n",
    "            key = d + '_R' + str(R) + '_K' + str(K) + '_RF'\n",
    "            inner_file = file_name + '/' + key + '.npy'\n",
    "            metrics[key] = np.load(inner_file, allow_pickle = True)\n",
    "            #nome_file = \"<insert your path>/\" + d + \"/\" + d + \"_metrics.npy\"\n",
    "            #np.save(nome_file, metrics)\n",
    "            nn += 1\n",
    "            \n",
    "metrics_tot = {}\n",
    "for d in dataset_list:\n",
    "    #insert your path here:\n",
    "    file_name = '<insert your path>/' + d + \"/\" + d + \"_metrics.npy\"\n",
    "    metrics = np.load(file_name, allow_pickle = True).item()\n",
    "\n",
    "    tss,f1,accuracy,precision,recall,specificity,npv,lr_p,lr_m,prev = [],[],[],[],[],[],[],[],[],[]\n",
    "\n",
    "    for key in list(metrics.keys()):\n",
    "        values = metrics[key].item()\n",
    "        #print(key)\n",
    "        #print(values)\n",
    "        cl = values['RF']\n",
    "        #print(cl)\n",
    "        th = cl[0.175]\n",
    "        #print(th)\n",
    "\n",
    "        tss.append(th['tss_ts'])\n",
    "        f1.append(th['f1score_ts'])\n",
    "        accuracy.append(th['accuracy_ts'])\n",
    "        precision.append(th['precision_ts'])\n",
    "        recall.append(th['recall_ts'])\n",
    "        specificity.append(th['specificity_ts'])\n",
    "        npv.append(th['npv_ts'])\n",
    "        \n",
    "        #Likelihood Ratios:\n",
    "        PRED_ts_bin = th['PRED_prob_ts'][:,1] > 0.175\n",
    "        PRED_ts_bin = PRED_ts_bin*1.\n",
    "        pos_LR, neg_LR = class_likelihood_ratios(th['yts'],PRED_ts_bin)\n",
    "        lr_p.append(pos_LR)\n",
    "        lr_m.append(neg_LR)\n",
    "        \n",
    "        #Prevalence:\n",
    "        tn, fp, fn, tp = confusion_matrix(th['yts'],PRED_ts_bin).ravel()\n",
    "        prev.append((tp+fn)/(tp+tn+fp+fn))\n",
    "        \n",
    "    metrics_tot[d] = {'TSS_F':tss, 'TSS_M':np.mean(tss),\n",
    "                      'F1_F':f1, 'F1_M':np.mean(f1),\n",
    "                      'Accuracy_F':accuracy, 'Accuracy_M':np.mean(accuracy),\n",
    "                      'Precision_F':precision, 'Precision_M':np.mean(precision),\n",
    "                      'Recall_F':recall, 'Recall_M':np.mean(recall),\n",
    "                      'Specificity_F':specificity, 'Specificity_M':np.mean(specificity),\n",
    "                      'NPV_F':npv, 'NPV_M':np.mean(npv),\n",
    "                      'LR+_F':lr_p, 'LR+_M':np.mean(lr_p),\n",
    "                      'LR-_F':lr_m, 'LR-_M':np.mean(lr_m),\n",
    "                      'PREV_F':prev, 'PREV_M':np.mean(prev),}   \n",
    "#print(metrics_tot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587ce66-fc47-4591-afae-5d61f4f52261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists of metrics to compute boxplots:\n",
    "list_ = ['BDG_PCT']*30 + ['All']*30 + ['with_12']*30 + ['without_12']*30\n",
    "nome_metrica = 'TSS_F'\n",
    "list_TSS = metrics_tot['BDG_PCT'][nome_metrica] + metrics_tot['All'][nome_metrica] + metrics_tot['with_12'][nome_metrica] +  metrics_tot['without_12'][nome_metrica]\n",
    "nome_metrica = 'Accuracy_F'\n",
    "list_Accuracy =  metrics_tot['BDG_PCT'][nome_metrica] + metrics_tot['All'][nome_metrica] + metrics_tot['with_12'][nome_metrica] +  metrics_tot['without_12'][nome_metrica] \n",
    "nome_metrica = 'Precision_F'\n",
    "list_Precision = metrics_tot['BDG_PCT'][nome_metrica] + metrics_tot['All'][nome_metrica] + metrics_tot['with_12'][nome_metrica] +  metrics_tot['without_12'][nome_metrica] \n",
    "nome_metrica = 'F1_F'\n",
    "list_F1= metrics_tot['BDG_PCT'][nome_metrica] + metrics_tot['All'][nome_metrica] + metrics_tot['with_12'][nome_metrica] +  metrics_tot['without_12'][nome_metrica]\n",
    "nome_metrica = 'Recall_F'\n",
    "list_Recall = metrics_tot['BDG_PCT'][nome_metrica] + metrics_tot['All'][nome_metrica] + metrics_tot['with_12'][nome_metrica] +  metrics_tot['without_12'][nome_metrica]\n",
    "nome_metrica = 'Specificity_F'\n",
    "list_Specificity = metrics_tot['BDG_PCT'][nome_metrica] + metrics_tot['All'][nome_metrica] + metrics_tot['with_12'][nome_metrica] +  metrics_tot['without_12'][nome_metrica] \n",
    "nome_metrica = 'NPV_F'\n",
    "list_NPV = metrics_tot['BDG_PCT'][nome_metrica] + metrics_tot['All'][nome_metrica] + metrics_tot['with_12'][nome_metrica] +  metrics_tot['without_12'][nome_metrica]\n",
    "nome_metrica = 'LR+_F'\n",
    "list_LR_p = metrics_tot['BDG_PCT'][nome_metrica] + metrics_tot['All'][nome_metrica] + metrics_tot['with_12'][nome_metrica] +  metrics_tot['without_12'][nome_metrica] \n",
    "nome_metrica = 'LR-_F'\n",
    "list_LR_m = metrics_tot['BDG_PCT'][nome_metrica] + metrics_tot['All'][nome_metrica] + metrics_tot['with_12'][nome_metrica] +  metrics_tot['without_12'][nome_metrica]\n",
    "\n",
    "df_metrics = pd.DataFrame([list_,list_TSS,list_Accuracy,list_Precision,list_F1,list_Recall,list_Specificity,list_NPV,list_LR_p,list_LR_m]).transpose()\n",
    "df_metrics.columns = ['Dataset','TSS','Accuracy','Precision','F1','Recall','Specificity','NPV','LR+','LR-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94cc92b-b9d5-45c7-8748-e3f3acb0d3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Use boxplots to visualize performances' distribution (not shown):\n",
    "metr = ['TSS_F','F1_F','Accuracy_F','Precision_F','Recall_F','Specificity_F','NPV_F','LR+_F','LR-_F']\n",
    "\n",
    "for m in metr:\n",
    "    print(m + ':')\n",
    "    vect = []\n",
    "    for d in dataset_list:\n",
    "        metrics_dataset = metrics_tot[d]\n",
    "        vect.append(metrics_dataset[m])\n",
    "    vect_df = np.transpose(pd.DataFrame(vect))\n",
    "    vect_df.columns = dataset_list\n",
    "    print(vect_df.describe())\n",
    "    sns.boxplot(vect_df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ec5ab-9826-4369-865d-187c3ef0a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplots (Figure3 and S6):\n",
    "def set_box_color(bp, color):\n",
    "    plt.setp(bp['boxes'], color=color)\n",
    "    plt.setp(bp['whiskers'], color=color)\n",
    "    plt.setp(bp['caps'], color=color)\n",
    "    plt.setp(bp['medians'], color=color)\n",
    "\n",
    "colors = ['#bebada','#80b1d3','#fdb462','#b3de69']\n",
    "\n",
    "group = 'Dataset'\n",
    "dataset_list = ['All','BDG_PCT','with_12','without_12']\n",
    "ngroup = len(dataset_list)\n",
    "metr_df_metrics = ['TSS','Accuracy','Precision','F1','Recall','Specificity','NPV','LR+','LR-']\n",
    "\n",
    "for m in list(metr_df_metrics):\n",
    "    column = m\n",
    "\n",
    "    grouped0 = df_metrics.groupby(group)\n",
    "\n",
    "    names0, vals0, x0 = [], [] ,[]\n",
    "\n",
    "    for i, (name, subdf) in enumerate(grouped0):\n",
    "        names0.append(name)\n",
    "        vals0.append((subdf[column]).tolist())\n",
    "        x0.append([4*(i)+1]*30)\n",
    "        \n",
    "    myorder = [1,0,2,3]\n",
    "    x0 = [x0[j] for j in myorder]\n",
    "    positions0 = [5,1,9,13]\n",
    "    bp0 = plt.boxplot(vals0, positions = positions0, widths=[1.2]*4)\n",
    "    ngroup = len(vals0)\n",
    "\n",
    "    i=0\n",
    "    for x, val in zip(x0, vals0):\n",
    "        plt.scatter(x, val, color=colors[i], alpha=0.4)\n",
    "        i+=1\n",
    "    #------------------------------------------------------------------------------------------------------- \n",
    "\n",
    "    nome_figura = 'Boxplot_' + m\n",
    "    if m == 'LR+':\n",
    "        plt.ylim(float(df_metrics[[m]].min())-0.5,float(df_metrics[[m]].max())+0.7)\n",
    "        yticks = np.linspace(1,5,5)\n",
    "    elif m == 'LR-':\n",
    "        plt.ylim(float(df_metrics[[m]].min())-0.1,float(df_metrics[[m]].max())+0.08)\n",
    "        yticks = np.linspace(0,1,5)\n",
    "    elif m == 'TSS':\n",
    "        plt.ylim(float(df_metrics[[m]].min())-0.1,1.1)\n",
    "        yticks = np.linspace(0,1,6)\n",
    "    elif m == 'Precision':\n",
    "        plt.ylim(float(df_metrics[[m]].min())-0.1,1.05)\n",
    "        yticks = np.linspace(0.7,1,4)\n",
    "    elif m == 'Recall':\n",
    "        plt.ylim(float(df_metrics[[m]].min())-0.1,1.05)\n",
    "        yticks = np.linspace(0.2,1,5)\n",
    "    elif m == 'NPV':\n",
    "        plt.ylim(float(df_metrics[[m]].min())-0.05,1.01)\n",
    "        yticks = np.linspace(0.8,1,5)\n",
    "    else:\n",
    "        plt.ylim(float(df_metrics[[m]].min())-0.1,1.05)\n",
    "        yticks = np.linspace(0.4,1,4)\n",
    "    #plt.legend(title='Random state',fontsize='8',loc='lower center', ncol = 3)\n",
    "    #labels = ['bw' if x=='bw200' else x for x in names0]\n",
    "    labels = ['BDG-PCT + All fetaures','BDG-PCT','BGD-PCT + 12 best features','12 best features']\n",
    "    \n",
    "    plt.xticks([5,1,9,13], labels,size=6.5)\n",
    "    plt.yticks(yticks,size=6.5)\n",
    "    plt.xlabel('Subset', fontsize=10)\n",
    "    if m=='F1':\n",
    "        plt.ylabel('F1-score', fontsize=10)\n",
    "    else:\n",
    "        plt.ylabel(m, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    set_box_color(bp0, '#636363') # colors are from http://colorbrewer2.org/\n",
    "\n",
    "    #colors = ['blue', 'green', 'purple', 'tan', 'pink', 'red']\n",
    "    #for patch, color in zip(bp0['boxes'], colors):\n",
    "    #    patch.set_facecolor(color)\n",
    "\n",
    "    #plt.title('Boxplots of ' + m , fontsize=15) \n",
    "    \n",
    "    #Save figure:\n",
    "    #nome_figura = '<insert your path>/' + nome_figura + '.png'\n",
    "    #plt.savefig(nome_figura,dpi = 300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146289f5-1788-47af-8e46-ea1fc7c6417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friedman Test: Paired samples\n",
    "# Non-parametric test used to determine whether there is a statistical difference between the medians of at least two groups \n",
    "metr = ['TSS_F','F1_F','Accuracy_F','Precision_F','Recall_F','Specificity_F','NPV_F','LR+_F','LR-_F']\n",
    "    \n",
    "# BDG_PCT vs All vs Best_with vs Best_without:\n",
    "print('\\n')\n",
    "print('Friedman Test for PAIRED samples - BDG_PCT vs All vs Best_with vs Best_without:\\n') #alpha = ' + str(round(0.05/4,3)) +' \n",
    "    \n",
    "for m in metr:\n",
    "    x2 = metrics_tot['All'] \n",
    "    x3 = metrics_tot['with_12'] \n",
    "    x1 = metrics_tot['BDG_PCT'] \n",
    "    x4 = metrics_tot['without_12'] \n",
    "\n",
    "    print(m + ': p = ', stats.friedmanchisquare(x1[m],x2[m],x3[m],x4[m])[1])\n",
    "    \n",
    "    #If the Friedman test is rejected, then each couple is tested with Wilcoxon test:\n",
    "    if stats.friedmanchisquare(x1[m],x2[m],x3[m],x4[m])[1] < 0.05:\n",
    "        print('Test fails for the couple: ')\n",
    "        print('The p-value corrected with the Bonferroni correction is: ',0.05/6)\n",
    "        if stats.wilcoxon(x1[m],x2[m])[1] < 0.05/6:\n",
    "            print('BDG_PCT vs All: p = ',stats.wilcoxon(x1[m],x2[m])[1])\n",
    "        if stats.wilcoxon(x1[m],x3[m])[1] < 0.05/6:\n",
    "            print('BDG_PCT vs Best with: p = ',stats.wilcoxon(x1[m],x3[m])[1])\n",
    "        if stats.wilcoxon(x1[m],x4[m])[1] < 0.05/6:\n",
    "            print('BDG_PCT vs Best without: p = ',stats.wilcoxon(x1[m],x4[m])[1])\n",
    "        if stats.wilcoxon(x2[m],x3[m])[1] < 0.05/6:\n",
    "            print('All vs Best with: p = ',stats.wilcoxon(x2[m],x3[m])[1])\n",
    "        if stats.wilcoxon(x2[m],x4[m])[1] < 0.05/6:\n",
    "            print('All vs Best without: p = ',stats.wilcoxon(x2[m],x4[m])[1])\n",
    "        if stats.wilcoxon(x3[m],x4[m])[1] < 0.05/6:\n",
    "            print('Best with vs Best without: p = ',stats.wilcoxon(x3[m],x4[m])[1])\n",
    "        print('-------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7328e-514e-4dde-8cef-b288d8fbc669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No other data set needs to be exported for following analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
