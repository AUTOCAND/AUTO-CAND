{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60d2865f-b1b7-4b63-bff9-afb7ea99d5f9",
   "metadata": {},
   "source": [
    "# <font color='Crimson'><b>MACHINE LEARNING ALGORITHMS</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264193ac-bf66-4d89-8c35-f4f02c028e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import class_likelihood_ratios\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896d4d7-2838-41fa-af2a-98e10d4c2024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import imputed and standardized training set:\n",
    "training_std = pd.read_csv('05_Training_standard.csv',sep=',',index_col=0)\n",
    "training_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15553a-563a-45a0-8e2a-f6bfdf809e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import imputed and standardized test set:\n",
    "test_std = pd.read_csv('05_Test_standard.csv',sep=',',index_col=0)\n",
    "test_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f424a12-93aa-4817-92ee-fa02d08f28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the two sets:\n",
    "training_std = shuffle(training_std,random_state=100)\n",
    "test_std = shuffle(test_std,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f79a6-912b-4058-922f-275cbd31e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split features from outcome in both sets:\n",
    "X_training = training_std.iloc[:,2:training_std.shape[1]].values\n",
    "y_training = training_std.iloc[:,0].values\n",
    "y_training = np.double(y_training)\n",
    "X_test = test_std.iloc[:,2:test_std.shape[1]].values\n",
    "y_test = test_std.iloc[:,0].values\n",
    "y_test = np.double(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f768b-c489-4f33-b27e-f395dc5de4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define useful functions for classification task:\n",
    "#TSS score:\n",
    "def my_tss_score(Y_training,probability_prediction,threshold):\n",
    "    \n",
    "    Y_predicted = probability_prediction > threshold\n",
    "    res = metrics_classification(Y_training > 0, Y_predicted, print_skills=False)    \n",
    "\n",
    "    return res['tss']\n",
    "\n",
    "#Classification metrics:\n",
    "def metrics_classification(y_real, y_pred, print_skills=True):\n",
    "\n",
    "    cm, far, pod, acc, hss, tss, fnfp, csi, tpr, tnr = classification_skills(y_real, y_pred)\n",
    "\n",
    "    if print_skills:\n",
    "        print ('confusion matrix')\n",
    "        print (cm)\n",
    "        print ('false alarm ratio       \\t', far)\n",
    "        print ('probability of detection\\t', pod)\n",
    "        print ('accuracy                \\t', acc)\n",
    "        print ('hss                     \\t', hss)\n",
    "        print ('tss                     \\t', tss)\n",
    "        print ('balance                 \\t', fnfp)\n",
    "        print ('csi                 \\t', csi)\n",
    "        print ('tpr                 \\t', tpr)\n",
    "        print ('tnr                 \\t', tnr)\n",
    "\n",
    "    balance_label = float(sum(y_real)) / y_real.shape[0]\n",
    "\n",
    "    return {\n",
    "        \"cm\": cm,\n",
    "        \"far\": far,\n",
    "        \"pod\": pod,\n",
    "        \"acc\": acc,\n",
    "        \"hss\": hss,\n",
    "        \"tss\": tss,\n",
    "        \"fnfp\": fnfp,\n",
    "        \"balance label\": balance_label,\n",
    "        \"csi\": csi,\n",
    "        \"tpr\": tpr,\n",
    "        \"tnr\": tnr}\n",
    "\n",
    "def classification_skills(y_real, y_pred):\n",
    "\n",
    "    cm = confusion_matrix(y_real, y_pred)\n",
    "\n",
    "    if cm.shape[0] == 1 and sum(y_real) == 0:\n",
    "        a = 0.\n",
    "        d = float(cm[0, 0])\n",
    "        b = 0.\n",
    "        c = 0.\n",
    "    elif cm.shape[0] == 1 and sum(y_real) == y_real.shape[0]:\n",
    "        a = float(cm[0, 0])\n",
    "        d = 0.\n",
    "        b = 0.\n",
    "        c = 0.\n",
    "    elif cm.shape[0] == 2:\n",
    "        a = float(cm[1, 1])\n",
    "        d = float(cm[0, 0])\n",
    "        b = float(cm[0, 1])\n",
    "        c = float(cm[1, 0])\n",
    "    TP = a\n",
    "    TN = d\n",
    "    FP = b\n",
    "    FN = c\n",
    "\n",
    "    if (TP + FP + FN + TN) == 0.:\n",
    "        if (TP + TN) == 0.:\n",
    "            acc = 0.  # float('NaN')\n",
    "        else:\n",
    "            acc = -100  # float('Inf')\n",
    "    else:\n",
    "        acc = (TP + TN) / (TP + FP + FN + TN)\n",
    "        \n",
    "\n",
    "    if TP + FN == 0.:\n",
    "        if TP == 0.:\n",
    "            tss_aux1 = 0.  # float('NaN')\n",
    "        else:\n",
    "            tss_aux1 = -100  # float('Inf')\n",
    "    else:\n",
    "        tss_aux1 = (TP / (TP + FN))\n",
    "\n",
    "    if (FP + TN) == 0.:\n",
    "        if FP == 0.:\n",
    "            tss_aux2 = 0.  # float('NaN')\n",
    "        else:\n",
    "            tss_aux2 = -100  # float('Inf')\n",
    "    else:\n",
    "        tss_aux2 = (FP / (FP + TN))\n",
    "\n",
    "    tss = tss_aux1 - tss_aux2\n",
    "\n",
    "    if ((TP + FN) * (FN + TN) + (TP + FP) * (FP + TN)) == 0.:\n",
    "        if (TP * TN - FN * FP) == 0:\n",
    "            hss = 0.  # float('NaN')\n",
    "        else:\n",
    "            hss = -100  # float('Inf')\n",
    "    else:\n",
    "        hss = 2 * (TP * TN - FN * FP) / ((TP + FN) *\n",
    "                                         (FN + TN) + (TP + FP) * (FP + TN))\n",
    "\n",
    "    if FP == 0.:\n",
    "        if FN == 0.:\n",
    "            fnfp = 0.  # float('NaN')\n",
    "        else:\n",
    "            fnfp = -100  # float('Inf')\n",
    "    else:\n",
    "        fnfp = FN / FP\n",
    "\n",
    "    if (TP + FN) == 0.:\n",
    "        if TP == 0.:\n",
    "            pod = 0  # float('NaN')\n",
    "        else:\n",
    "            pod = -100  # float('Inf')\n",
    "    else:\n",
    "        pod = TP / (TP + FN)\n",
    "\n",
    "\n",
    "    if (TP + FP) == 0.:\n",
    "        if FP == 0.:\n",
    "            far = 0.  # float('NaN')\n",
    "        else:\n",
    "            far = -100  # float('Inf')\n",
    "    else:\n",
    "        far = FP / (TP + FP)\n",
    "\n",
    "    #acc = (a + d) / (a + b + c + d)\n",
    "    tpr = tss_aux1  # a / (a + b)\n",
    "    tnr = 1-tss_aux2  # d / (d + c)\n",
    "    #wtpr = a / (a + b) * (a + c) / (a + b + c + d) + d / (c + d) * (b + d) / (a + b + c + d)\n",
    "    #pacc = a / (a + c)\n",
    "    #nacc = d / (b + d)\n",
    "    #wacc = a / (a + c) * (a + c) / (a + b + c + d) + d / (b + d) * (b + d) / (a + b + c + d)\n",
    "\n",
    "    # if the cm has a row or a column equal to 0, we have bad tss\n",
    "    if TP+FN == 0 or TN+FP == 0 or TP+FP == 0 or TN+FN == 0:\n",
    "        tss = 0\n",
    "    if TP+FP+FN==0:\n",
    "        csi = 0\n",
    "    else:\n",
    "        csi = TP/(TP+FP+FN)\n",
    "\n",
    "    return cm.tolist(), far, pod, acc, hss, tss, fnfp, csi, tpr, tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1ffca-7fd4-45e0-9137-9fcdd43f79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define useful fuctions for cross-validation:\n",
    "def build_grid(p):\n",
    "    #Parameters initialization:\n",
    "    model_name = None if p['model_name'] == None else p['model_name']\n",
    "    GRID = None \n",
    "        \n",
    "    if model_name == \"RF\":\n",
    "        GRID = {'n_estimators': p['n_estimators'],\n",
    "                'max_features': p['max_features'],\n",
    "                'max_depth': p['max_depth'],\n",
    "                'criterion': p['criterion']\n",
    "               }  \n",
    "    \n",
    "    #other models can be added here\n",
    "\n",
    "    return GRID\n",
    "\n",
    "\n",
    "def initiate_p(p):\n",
    "    #Ranges for hyperparameter search:\n",
    "    if p['model_name'] == 'RF':\n",
    "        p['estimator'] = RandomForestClassifier(random_state=100)\n",
    "        p['n_estimators'] = [100,200,300,400,500] \n",
    "        p['max_features'] = [None, 'sqrt', 'log2'] \n",
    "        p['max_depth'] = [4,5,6,7,8]\n",
    "        p['criterion'] = ['gini','entropy']\n",
    "        \n",
    "    #other models can be added here\n",
    "\n",
    "    #All:\n",
    "    p['cv'] = 10\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def GridSearch_(X,y,p):\n",
    "    #Parameters initialization:\n",
    "    estimator = None if p['estimator'] == None else p['estimator']\n",
    "    cv = 10 if p['cv'] == None else p['cv']\n",
    "    grid = None if p['grid'] == None else p['grid']\n",
    "    tau = 0.5 if p['threshold'] == None else p['threshold']\n",
    "    \n",
    "    if(estimator == None or grid == None):\n",
    "        return None\n",
    "\n",
    "    #1st step: select the best hyperparameters\n",
    "    CV = GridSearchCV(estimator     = estimator,\n",
    "                        param_grid  = grid,\n",
    "                        scoring     =  make_scorer(my_tss_score,threshold=tau,needs_proba=True), #TSS score\n",
    "                        #refit       = 'roc_auc',\n",
    "                        cv          = cv,\n",
    "                        verbose     = 0,\n",
    "                        n_jobs      = 20,\n",
    "                        return_train_score=True)\n",
    "    CV_H = CV.fit(X,y)\n",
    "    \n",
    "    return CV_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42bf0b-e99e-49ed-9633-863157e2fe92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Train the classifiers and evaluate performances both on training and test sets:\n",
    "\n",
    "#Range of threshold to evaluate:\n",
    "thresholds = np.linspace(0.0,0.5,11)\n",
    "\n",
    "#Classifiers to train:\n",
    "classifiers = ['LogisticRegL1','LogisticRegL2','RF'] \n",
    "metrics_cl = {}\n",
    "\n",
    "for cl in classifiers:\n",
    "    p = {}\n",
    "    p['model_name'] = cl\n",
    "    p = initiate_p(p)\n",
    "    print('Model name:', p['model_name'])\n",
    "    \n",
    "    metrics_th = {}\n",
    "    \n",
    "    for th in tqdm(thresholds):\n",
    "        p['threshold'] = th\n",
    "        print('Threshold:', p['threshold'])\n",
    "        \n",
    "        performance = {}\n",
    "        #---------------------------------------------\n",
    "        #Hyperparameters search and model fit on training set:    \n",
    "        if p['model_name'] == 'LogisticRegL1':\n",
    "            score = make_scorer(my_tss_score,threshold=th,needs_proba=True)\n",
    "            fit = LogisticRegressionCV(cv = 10,random_state = 100,penalty='l1',solver='liblinear',scoring=score).fit(X_training,y_training) \n",
    "        \n",
    "        elif p['model_name'] == 'LogisticRegL2':\n",
    "            score = make_scorer(my_tss_score,threshold=th,needs_proba=True)\n",
    "            fit = LogisticRegressionCV(cv = 10,random_state = 100,penalty='l2',scoring=score).fit(X_training,y_training) \n",
    "               \n",
    "        elif p['model_name'] == 'RF':\n",
    "            p['grid'] = build_grid(p)\n",
    "            p['GS_RF'] = GridSearch_(X_training, y_training, p)\n",
    "            max_depth = p['GS_RF'].best_params_['max_depth']\n",
    "            n_estimators = p['GS_RF'].best_params_['n_estimators']\n",
    "            criterion = p['GS_RF'].best_params_['criterion']\n",
    "            max_features = p['GS_RF'].best_params_['max_features']\n",
    "            print(p['GS_RF'].best_params_)\n",
    "            performance['best_hyper'] = p['GS_RF'].best_params_\n",
    "            fit = RandomForestClassifier(max_depth=max_depth,n_estimators=n_estimators,criterion=criterion,\n",
    "                                         max_features=max_features,random_state=100).fit(X_training, y_training) \n",
    "            \n",
    "        #other models can be added here\n",
    "            \n",
    "        #---------------------------------------------\n",
    "        #Predict on training set:\n",
    "        PRED_prob_tr = fit.predict_proba(X_training)\n",
    "        PRED_tr_yes = PRED_prob_tr[:,1] \n",
    "\n",
    "        PRED_tr_bin = PRED_tr_yes > th\n",
    "        PRED_tr_bin = PRED_tr_bin*1.\n",
    "        \n",
    "        performance['ytr'] = y_training\n",
    "        performance['PRED_prob_tr'] = PRED_prob_tr\n",
    "\n",
    "        #Compute evaluation metrics on training set:\n",
    "        print(confusion_matrix(y_training,PRED_tr_bin))\n",
    "        tn_tr, fp_tr, fn_tr, tp_tr = confusion_matrix(y_training, PRED_tr_bin).ravel()\n",
    "        spec_tr = tn_tr / (tn_tr+fp_tr)\n",
    "        f1_tr = f1_score(y_training, PRED_tr_bin,average = 'weighted')\n",
    "        acc_tr = accuracy_score(y_training, PRED_tr_bin)\n",
    "        prec_tr = precision_score(y_training, PRED_tr_bin,average = 'weighted')\n",
    "        recall_tr = recall_score(y_training, PRED_tr_bin) \n",
    "        npv_tr = tn_tr / (tn_tr+fn_tr)\n",
    "\n",
    "        performance['tss_tr'] = recall_tr+spec_tr-1\n",
    "        performance['f1score_tr'] = f1_tr\n",
    "        performance['accuracy_tr'] = acc_tr\n",
    "        performance['precision_tr'] = prec_tr\n",
    "        performance['recall_tr'] = recall_tr\n",
    "        performance['specificity_tr'] = spec_tr \n",
    "        performance['npv_tr'] = npv_tr \n",
    "        \n",
    "        #---------------------------------------------\n",
    "        #Predict on test set:\n",
    "        PRED_prob_ts = fit.predict_proba(X_test)\n",
    "        PRED_ts_yes = PRED_prob_ts[:,1]\n",
    "\n",
    "        PRED_ts_bin = PRED_ts_yes>th\n",
    "        PRED_ts_bin = PRED_ts_bin*1.\n",
    "        \n",
    "        performance['yts'] = y_test\n",
    "        performance['PRED_prob_ts'] = PRED_prob_ts\n",
    "\n",
    "        #Compute evaluation metrics on test set:\n",
    "        print(confusion_matrix(y_test,PRED_ts_bin))\n",
    "        tn_ts, fp_ts, fn_ts, tp_ts = confusion_matrix(y_test,PRED_ts_bin).ravel()\n",
    "        spec_ts = tn_ts / (tn_ts+fp_ts)\n",
    "        f1_ts = f1_score(y_test, PRED_ts_bin,average = 'weighted')\n",
    "        acc_ts = accuracy_score(y_test, PRED_ts_bin)\n",
    "        prec_ts = precision_score(y_test, PRED_ts_bin,average = 'weighted')\n",
    "        recall_ts = recall_score(y_test, PRED_ts_bin)  \n",
    "        npv_ts = tn_ts / (tn_ts+fn_ts)\n",
    "\n",
    "        performance['tss_ts'] = recall_ts+spec_ts-1\n",
    "        performance['f1score_ts'] = f1_ts\n",
    "        performance['accuracy_ts'] = acc_ts\n",
    "        performance['precision_ts'] = prec_ts\n",
    "        performance['recall_ts'] = recall_ts\n",
    "        performance['specificity_ts'] = spec_ts\n",
    "        performance['npv_ts'] = npv_ts\n",
    "        #---------------------------------------------\n",
    "        metrics_th[th] = performance\n",
    "    #---------------------------------------------\n",
    "    #Performances at each treshold:\n",
    "    metrics_cl[cl] = metrics_th\n",
    "    #Save on file:\n",
    "    #nome_file = '<insert_your_path>/' + cl + '_Performances.npy'\n",
    "    #np.save(nome_file, metrics_th)\n",
    "#Save on file:    \n",
    "#nome_file = '<insert_your_path>/Complete_Performances.npy'\n",
    "#np.save(nome_file, metrics_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec34a7-0850-417e-a920-50bef53a9b23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Likelihood Ratios:\n",
    "print('LR L1:')\n",
    "for th in thresholds:\n",
    "    print(th)\n",
    "\n",
    "    tn_tr, fp_tr, fn_tr, tp_tr = confusion_matrix(size_LRL1[th]['ytr'],size_LRL1[th]['PRED_bin_tr']).ravel()\n",
    "    pos_LR_tr, neg_LR_tr = class_likelihood_ratios(size_LRL1[th]['ytr'],size_LRL1[th]['PRED_bin_tr'])\n",
    "    print(f\"training set LR+: {pos_LR_tr:.3f}\")\n",
    "    print(f\"training set LR-: {neg_LR_tr:.3f}\")\n",
    "\n",
    "    tn_ts, fp_ts, fn_ts, tp_ts = confusion_matrix(size_LRL1[th]['yts'],size_LRL1[th]['PRED_ts_bin']).ravel()\n",
    "    pos_LR_ts, neg_LR_ts = class_likelihood_ratios(size_LRL1[th]['yts'],size_LRL1[th]['PRED_ts_bin'])\n",
    "    print(f\"Test set LR+: {pos_LR_ts:.3f}\")\n",
    "    print(f\"Test set LR-: {neg_LR_ts:.3f}\")\n",
    "\n",
    "print(' ')\n",
    "print('LR L2:')\n",
    "for th in thresholds:\n",
    "    print(th)\n",
    "\n",
    "    tn_tr, fp_tr, fn_tr, tp_tr = confusion_matrix(size_LRL2[th]['ytr'],size_LRL2[th]['PRED_bin_tr']).ravel()\n",
    "    pos_LR_tr, neg_LR_tr = class_likelihood_ratios(size_LRL2[th]['ytr'],size_LRL2[th]['PRED_bin_tr'])\n",
    "    print(f\"training set LR+: {pos_LR_tr:.3f}\")\n",
    "    print(f\"training set LR-: {neg_LR_tr:.3f}\")\n",
    "\n",
    "    tn_ts, fp_ts, fn_ts, tp_ts = confusion_matrix(size_LRL2[th]['yts'],size_LRL2[th]['PRED_ts_bin']).ravel()\n",
    "    pos_LR_ts, neg_LR_ts = class_likelihood_ratios(size_LRL2[th]['yts'],size_LRL2[th]['PRED_ts_bin'])\n",
    "    print(f\"Test set LR+: {pos_LR_ts:.3f}\")\n",
    "    print(f\"Test set LR-: {neg_LR_ts:.3f}\")\n",
    "    \n",
    "print(' ')\n",
    "print('RF:')\n",
    "for th in thresholds:\n",
    "    print(th)\n",
    "\n",
    "    tn_tr, fp_tr, fn_tr, tp_tr = confusion_matrix(size_RF[th]['ytr'],size_RF[th]['PRED_bin_tr']).ravel()\n",
    "    pos_LR_tr, neg_LR_tr = class_likelihood_ratios(size_RF[th]['ytr'],size_RF[th]['PRED_bin_tr'])\n",
    "    print(f\"training set LR+: {pos_LR_tr:.3f}\")\n",
    "    print(f\"training set LR-: {neg_LR_tr:.3f}\")\n",
    "\n",
    "    tn_ts, fp_ts, fn_ts, tp_ts = confusion_matrix(size_RF[th]['yts'],size_RF[th]['PRED_ts_bin']).ravel()\n",
    "    pos_LR_ts, neg_LR_ts = class_likelihood_ratios(size_RF[th]['yts'],size_RF[th]['PRED_ts_bin'])\n",
    "    print(f\"Test set LR+: {pos_LR_ts:.3f}\")\n",
    "    print(f\"Test set LR-: {neg_LR_ts:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334f7dc-f81f-45a7-83f2-8733bd5b9a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fagan's nomograms (Figure S2):\n",
    "Pmax = .999\n",
    "Pmin = .001\n",
    "Omax = np.ceil(np.log(Pmax/(1-Pmax)))\n",
    "Omin = np.floor(np.log(Pmin/(1-Pmin)))\n",
    "\n",
    "Pticks = np.sort(np.concatenate([\n",
    "    np.arange(.1, .999, .1),\n",
    "    np.arange(.08, 0, -.02),\n",
    "    np.arange(.01, 0, -.002),\n",
    "    np.arange(.92, .999, +.02),\n",
    "    np.arange(.99, .999, +.002),\n",
    "    [.001, .999]\n",
    "]))\n",
    "\n",
    "Oticks = Pticks / (1 - Pticks)\n",
    "\n",
    "\n",
    "Lticks = [10 ** int(i) for i in np.arange(-4, 4.1, 1)]\n",
    "\n",
    "\n",
    "def Fagan(LR='pos',draw=[],cl='RF',th=0.1):\n",
    "    fig, ax = plt.subplots(figsize=(5,10), dpi=100)\n",
    "    ax.set_xlim(-1.2, 1.2)\n",
    "    ax.set_ylim(Omin, Omax)\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax.axvline(-1, c=\"k\")\n",
    "    ax.scatter([-1 for _ in range(len(Pticks))], np.log(Oticks), marker=\"_\", c=\"k\", s=100)\n",
    "    for Otick, Ptick in zip(Oticks, Pticks):\n",
    "        ax.text(\n",
    "            -1.05, np.log(Otick),\n",
    "            f\"${Ptick:.0%}$\" if Ptick<=.99 and Ptick>=.01 else f\"${Ptick:.1%}$\",\n",
    "            fontsize=7, va=\"center\", ha=\"right\"\n",
    "        )\n",
    "\n",
    "    ax.axvline(+1, c=\"k\")\n",
    "    ax.scatter([+1 for _ in range(len(Pticks))], np.log(1/Oticks), marker=\"_\", c=\"k\", s=100)\n",
    "    for Otick, Ptick in zip(Oticks, Pticks):\n",
    "        ax.text(\n",
    "            +1.05, np.log(Otick),\n",
    "            f\"${1-Ptick:.0%}$\" if Ptick<=.99 and Ptick>=.01 else f\"${1-Ptick:.1%}$\",\n",
    "            fontsize=7, va=\"center\", ha=\"left\"\n",
    "        )\n",
    "\n",
    "    Ops = []\n",
    "    for Ltick in Lticks:\n",
    "        Op = 1 / (np.sqrt(Ltick) + 1)\n",
    "        Ops.append(Op)\n",
    "        ax.scatter(0, np.log(Op/(1-Op)), marker=\"_\", c=\"k\", s=100)\n",
    "        ax.text(+.05, np.log(Op/(1-Op)), f\"${Ltick}$\", fontsize=7, c=\"k\", va=\"center\", ha=\"left\")\n",
    "        ax.text(-.05, np.log(Op/(1-Op)), f\"${Ltick}$\", fontsize=7, c=\"k\", va=\"center\", ha=\"right\")\n",
    "        #minors = [Ltick-10**int(np.log10(Ltick)-1)*i for i in range(9)]\n",
    "        #for minor in minors:\n",
    "            #if minor < .001:\n",
    "                #continue\n",
    "            #Op = 1 / (np.sqrt(minor) + 1)\n",
    "            #ax.scatter(0, np.log(Op/(1-Op)), marker=\"_\", c=\"k\", s=50)\n",
    "    ax.plot([0,0], [np.log(Ops[0]/(1-Ops[0])), np.log(Ops[-1]/(1-Ops[-1]))], c=\"k\", ls=\"-\")\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(\"Fagan's nomogram - \" + cl , fontsize=12)\n",
    "    ax.text(-1.3, 0, \"Pre-test Probability (%)\", ha=\"center\", va=\"center\", fontsize=12, rotation=90)\n",
    "    ax.text(+1.35, 0, \"Post-test Probability (%)\", ha=\"center\", va=\"center\", fontsize=12, rotation=90)\n",
    "    if LR=='neg':\n",
    "        ax.text(0, np.log(Ops[0]/(1-Ops[0]))+.5, \"Negative Likelihood Ratio\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "    else: \n",
    "        ax.text(0, np.log(Ops[0]/(1-Ops[0]))+.5, \"Positive Likelihood Ratio\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "    \n",
    "    for line in draw:\n",
    "            Opo = line[1] * (line[0]/(1-line[0]))\n",
    "            Ppo = Opo/(1+Opo)\n",
    "            print(f\"Pr:{line[0]:>7.2%}  LR:{line[1]:>7.2f}  Po:{Ppo:>7.2%}\")\n",
    "            ax.plot(\n",
    "                [-1, 1], [np.log(line[0]/(1-line[0])), np.log((1-line[0])/(line[0]*line[1]))], color=line[2],linestyle=line[3]\n",
    "            )\n",
    "        \n",
    "    #nome_figura = '<insert_your_path>/' + cl + '_Fagan_' + LR + '_th_' +str(th) +'.png'\n",
    "    #plt.savefig(nome_figura,dpi=300)\n",
    "\n",
    "    plt.show();\n",
    "\n",
    "\n",
    "#Figure S2, A: PLR L1, threshold=0.1:\n",
    "Fagan( LR = 'pos',\n",
    "       draw=[\n",
    "             [.102, 1.884,'#80b1d3','-'], #prevalence training set, LR+, color, line style\n",
    "             [.102, 1.834,'#fb8072','--']  #prevalence test set, LR+,  color, line style\n",
    "            ],\n",
    "      cl = 'LR L1',\n",
    "      th = 0.1\n",
    "     )\n",
    "\n",
    "\n",
    "Fagan( LR = 'neg',\n",
    "       draw=[\n",
    "             [.102, 0.558, '#80b1d3','-'], #prevalence training set, LR-\n",
    "             [.102, 0.581,'#fb8072','--']  #prevalence test set, LR-\n",
    "            ],\n",
    "      cl = 'LR L1',\n",
    "      th = 0.1\n",
    "     )\n",
    "\n",
    "#Figure S2, B: PLR L2, threshold=0.1:\n",
    "Fagan( LR = 'pos',\n",
    "       draw=[\n",
    "             [.102, 1.804,'#80b1d3','-'], #prevalence training set, LR+\n",
    "             [.102, 1.764,'#fb8072','--']  #prevalence test set, LR+\n",
    "            ],\n",
    "      cl = 'LR L2',\n",
    "      th = 0.1\n",
    "     )\n",
    "\n",
    "\n",
    "Fagan( LR = 'neg',\n",
    "       draw=[\n",
    "             [.102, 0.503,'#80b1d3','-'], #prevalence training set, LR-\n",
    "             [.102, 0.528,'#fb8072','--']  #prevalence test set, LR-\n",
    "            ],\n",
    "      cl = 'LR L2',\n",
    "      th = 0.1\n",
    "     )\n",
    "\n",
    "#Figure S2, C: RF, threshold=0.1:\n",
    "Fagan( LR = 'pos',\n",
    "       draw=[\n",
    "             [.102, 2.774,'#80b1d3','-'], #prevalence training set, LR+\n",
    "             [.102, 1.702,'#fb8072','--']  #prevalence test set, LR+\n",
    "            ],\n",
    "      cl = 'RF',\n",
    "      th = 0.1\n",
    "     )\n",
    "\n",
    "\n",
    "Fagan( LR = 'neg',\n",
    "       draw=[\n",
    "             [.102, 0.033,'#80b1d3','-'], #prevalence training set, LR-\n",
    "             [.102, 0.462,'#fb8072','--']  #prevalence test set, LR-\n",
    "            ],\n",
    "      cl = 'RF',\n",
    "      th = 0.1\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984cb7f1-47c3-4e24-9cf7-0105224b9edf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Train again the best classifier chosen, with StratifiedKFold (for more than one shuffle) and evaluate performances on validation set:\n",
    "#Define the number of shuffles:\n",
    "R = 3\n",
    "#Define the number of folds:\n",
    "K = 10\n",
    "\n",
    "#Range of threshold to evaluate:\n",
    "thresholds = np.linspace(0.0,0.5,11)\n",
    "#Best classifier:\n",
    "classifiers = ['RF'] #LogisticRegL1 #LogisticRegL2\n",
    "\n",
    "#Shuffle data in order to test stability of the model:\n",
    "#In order to guarantee reproducibility of results, the random states are always the numbers 0,1,2.\n",
    "for r in range(0,R):\n",
    "    X,y = shuffle(X_training,y_training,random_state = r) \n",
    "    #------------------------------------------------------------------------------------------\n",
    "    # CROSS-VALIDATION: K-Fold\n",
    "    kf = StratifiedKFold(n_splits=K, shuffle=False)\n",
    "    #------------------------------------------------------------------------------------------\n",
    "    Kfold = 0\n",
    "    \n",
    "    for train_index, validation_index in kf.split(X,y):\n",
    "        \n",
    "        X_training, X_validation = X[train_index], X[validation_index]\n",
    "        y_training, y_validation = y[train_index], y[validation_index]\n",
    "        \n",
    "        for cl in classifiers:\n",
    "            metrics_cl = {}\n",
    "            \n",
    "            #metrics_cl['train_index'] = train_index\n",
    "            #metrics_cl['validation_index'] = validation_index\n",
    "            \n",
    "            p = {}\n",
    "            p['model_name'] = cl\n",
    "            p = initiate_p(p)\n",
    "            print('Model name:', p['model_name'])\n",
    "\n",
    "            metrics_th = {}\n",
    "\n",
    "            for th in tqdm(thresholds):\n",
    "                p['threshold'] = th\n",
    "                print('Threshold:', p['threshold'])\n",
    "\n",
    "                performance = {}\n",
    "                #---------------------------------------------\n",
    "                #Hyperparameters search and model fit on training set:    \n",
    "                if p['model_name'] == 'LogisticRegL1':\n",
    "                    score = make_scorer(my_tss_score,threshold=th,needs_proba=True)\n",
    "                    fit = LogisticRegressionCV(cv = 10,random_state = 100,penalty='l1',solver='liblinear',\n",
    "                                               scoring=score).fit(X_training, y_training) \n",
    "\n",
    "                elif p['model_name'] == 'LogisticRegL2':\n",
    "                    score = make_scorer(my_tss_score,threshold=th,needs_proba=True)\n",
    "                    fit = LogisticRegressionCV(cv = 10,random_state = 100,penalty='l2',scoring=score).fit(X_training, y_training) \n",
    "\n",
    "                elif p['model_name'] == 'RF':\n",
    "                    p['grid'] = build_grid(p)\n",
    "                    p['GS_RF'] = GridSearch_(X_training, y_training, p)\n",
    "                    max_depth = p['GS_RF'].best_params_['max_depth']\n",
    "                    n_estimators = p['GS_RF'].best_params_['n_estimators']\n",
    "                    criterion = p['GS_RF'].best_params_['criterion']\n",
    "                    max_features = p['GS_RF'].best_params_['max_features']\n",
    "                    print(p['GS_RF'].best_params_)\n",
    "                    performance['best_hyper'] = p['GS_RF'].best_params_\n",
    "                    fit = RandomForestClassifier(max_depth=max_depth,n_estimators=n_estimators,criterion=criterion,\n",
    "                                                 max_features=max_features,random_state=100).fit(X_training, y_training) \n",
    "                    \n",
    "                #other models can be added here\n",
    "                    \n",
    "                #---------------------------------------------\n",
    "                #Predict on training set:\n",
    "                PRED_prob_tr = fit.predict_proba(X_training)\n",
    "                PRED_tr_yes = PRED_prob_tr[:,1] \n",
    "\n",
    "                PRED_tr_bin = PRED_tr_yes > th\n",
    "                PRED_tr_bin = PRED_tr_bin*1.\n",
    "\n",
    "                performance['y_training'] = y_training\n",
    "                performance['PRED_prob_tr'] = PRED_prob_tr\n",
    "\n",
    "                #Compute evaluation metrics on training set:\n",
    "                print(confusion_matrix(y_training,PRED_tr_bin))\n",
    "                tn_tr, fp_tr, fn_tr, tp_tr = confusion_matrix(y_training, PRED_tr_bin).ravel()\n",
    "                spec_tr = tn_tr / (tn_tr+fp_tr)\n",
    "                f1_tr = f1_score(y_training, PRED_tr_bin,average = 'weighted')\n",
    "                acc_tr = accuracy_score(y_training, PRED_tr_bin)\n",
    "                prec_tr = precision_score(y_training, PRED_tr_bin,average = 'weighted')\n",
    "                recall_tr = recall_score(y_training, PRED_tr_bin) \n",
    "                npv_tr = tn_tr / (tn_tr+fn_tr)\n",
    "\n",
    "                performance['tss_tr'] = recall_tr+spec_tr-1\n",
    "                performance['f1score_tr'] = f1_tr\n",
    "                performance['accuracy_tr'] = acc_tr\n",
    "                performance['precision_tr'] = prec_tr\n",
    "                performance['recall_tr'] = recall_tr\n",
    "                performance['specificity_tr'] = spec_tr \n",
    "                performance['npv_tr'] = npv_tr \n",
    "\n",
    "                #---------------------------------------------\n",
    "                #Predict on validation set:\n",
    "                PRED_prob_ts = fit.predict_proba(X_validation)\n",
    "                PRED_ts_yes = PRED_prob_ts[:,1]\n",
    "\n",
    "                PRED_ts_bin = PRED_ts_yes>th\n",
    "                PRED_ts_bin = PRED_ts_bin*1.\n",
    "\n",
    "                performance['yts'] = y_validation\n",
    "                performance['PRED_prob_ts'] = PRED_prob_ts\n",
    "\n",
    "                #Compute evaluation metrics on validation set:\n",
    "                print(confusion_matrix(y_validation,PRED_ts_bin))\n",
    "                tn_ts, fp_ts, fn_ts, tp_ts = confusion_matrix(y_validation,PRED_ts_bin).ravel()\n",
    "                spec_ts = tn_ts / (tn_ts+fp_ts)\n",
    "                f1_ts = f1_score(y_validation, PRED_ts_bin,average = 'weighted')\n",
    "                acc_ts = accuracy_score(y_validation, PRED_ts_bin)\n",
    "                prec_ts = precision_score(y_validation, PRED_ts_bin,average = 'weighted')\n",
    "                recall_ts = recall_score(y_validation, PRED_ts_bin)  \n",
    "                npv_ts = tn_ts / (tn_ts+fn_ts)\n",
    "\n",
    "                performance['tss_ts'] = recall_ts+spec_ts-1\n",
    "                performance['f1score_ts'] = f1_ts\n",
    "                performance['accuracy_ts'] = acc_ts\n",
    "                performance['precision_ts'] = prec_ts\n",
    "                performance['recall_ts'] = recall_ts\n",
    "                performance['specificity_ts'] = spec_ts\n",
    "                performance['npv_ts'] = npv_ts \n",
    "                #---------------------------------------------\n",
    "                metrics_th[th] = performance\n",
    "            #---------------------------------------------\n",
    "            #Performances at each treshold:\n",
    "            metrics_cl[cl] = metrics_th\n",
    "            #---------------------------------------------\n",
    "            # Save on file:                \n",
    "            #nome_file = \"<insert your path>/R\"+ str(r) + \"_K\" + str(Kfold) + '_' + p['model_name'] + '.npy'\n",
    "            #np.save(nome_file, metrics_cl)\n",
    "            Kfold += 1\n",
    "        #---------------------------------------------\n",
    "    p_shuffle = {}\n",
    "    p_shuffle['R'] = r\n",
    "    #------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e1e78-1596-4b4c-ac66-1faefd77f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No data set needs to be exported for following analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
